---
# Source: llm-engine/templates/service_template_config_map.yaml
# THIS FILE IS AUTOGENERATED USING `just autogen-templates`. PLEASE EDIT THE GOTEMPLATE FILE IN THE HELM CHART!!!
apiVersion: v1
kind: ConfigMap
metadata:
  name: llm-engine-service-template-config
  labels:
    team: infra
    product: llm-engine
    helm.sh/chart: llm-engine-0.1.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
    tags.datadoghq.com/version: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
    tags.datadoghq.com/env: circleci
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-weight": "-2"
data:
  deployment-triton-enhanced-runnable-image-async-cpu.yaml: |-
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: ${RESOURCE_NAME}
      namespace: ${NAMESPACE}
      labels:
        user_id: ${OWNER}
        team: ${TEAM}
        product: ${PRODUCT}
        created_by: ${CREATED_BY}
        owner: ${OWNER}
        env: circleci
        managed-by: llm-engine
        use_scale_llm_engine_endpoint_network_policy: "true"
        tags.datadoghq.com/env: circleci
        tags.datadoghq.com/version: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
        tags.datadoghq.com/service: ${ENDPOINT_NAME}
        endpoint_id: ${ENDPOINT_ID}
        endpoint_name: ${ENDPOINT_NAME}
      annotations:
        celery.scaleml.autoscaler/queue: ${QUEUE}
        celery.scaleml.autoscaler/broker: ${BROKER_NAME}
        celery.scaleml.autoscaler/taskVisibility: "VISIBILITY_24H"
        celery.scaleml.autoscaler/perWorker: "${PER_WORKER}"
        celery.scaleml.autoscaler/minWorkers: "${MIN_WORKERS}"
        celery.scaleml.autoscaler/maxWorkers: "${MAX_WORKERS}"
    spec:
      strategy:
        type: RollingUpdate
        rollingUpdate:
          maxSurge: 1
          maxUnavailable: 0
      replicas: ${MIN_WORKERS}
      selector:
        matchLabels:
          app: ${RESOURCE_NAME}
          version: v1
      template:
        metadata:
          labels:
            app: ${RESOURCE_NAME}
            user_id: ${OWNER}
            team: ${TEAM}
            product: ${PRODUCT}
            created_by: ${CREATED_BY}
            owner: ${OWNER}
            env: circleci
            managed-by: llm-engine
            use_scale_llm_engine_endpoint_network_policy: "true"
            tags.datadoghq.com/env: circleci
            tags.datadoghq.com/version: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
            tags.datadoghq.com/service: ${ENDPOINT_NAME}
            endpoint_id: ${ENDPOINT_ID}
            endpoint_name: ${ENDPOINT_NAME}
            sidecar.istio.io/inject: "false"  # TODO: switch to scuttle
            version: v1
          annotations:
            ad.datadoghq.com/main.logs: '[{"service": "${ENDPOINT_NAME}", "source": "python"}]'
            kubernetes.io/change-cause: "${CHANGE_CAUSE_MESSAGE}"
        spec:
          affinity:
            podAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 1
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: app
                      operator: In
                      values:
                      - ${RESOURCE_NAME}
                  topologyKey: kubernetes.io/hostname
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: ${IMAGE_HASH}
                      operator: In
                      values:
                      - "True"
                  topologyKey: kubernetes.io/hostname
          terminationGracePeriodSeconds: 600
          serviceAccount: default
          nodeSelector:
            node-lifecycle: normal
          priorityClassName: ${PRIORITY}
          containers:
            - name: celery-forwarder
              image: 000000000000.dkr.ecr.us-west-2.amazonaws.com/llm-engine:${FORWARDER_IMAGE_TAG}
              imagePullPolicy: IfNotPresent
              command:
                - /usr/bin/dumb-init
                - --
                - ddtrace-run
                - run-service
                - --config
                - /workspace/server/llm_engine_server/inference/configs/${FORWARDER_CONFIG_FILE_NAME}
                - --queue
                - "${QUEUE}"
                - --task-visibility
                - "VISIBILITY_24H"
                - --set
                - "forwarder.model.args.predict_route=${PREDICT_ROUTE}"
                - --set
                - "forwarder.model.args.healthcheck_route=${HEALTHCHECK_ROUTE}"
                - --concurrency
                - "${PER_WORKER}"
              env:
                - name: DATADOG_TRACE_ENABLED
                  value: "${DATADOG_TRACE_ENABLED}"
                - name: DD_SERVICE
                  value: "${ENDPOINT_NAME}"
                - name: DD_ENV
                  value: circleci
                - name: DD_VERSION
                  value: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
                - name: DD_AGENT_HOST
                  valueFrom:
                    fieldRef:
                      fieldPath: status.hostIP
                - name: AWS_PROFILE
                  value: "${AWS_ROLE}"
                - name: RESULTS_S3_BUCKET
                  value: "${RESULTS_S3_BUCKET}"
                - name: BASE_PATH
                  value: "/workspace"
                - name: ML_INFRA_SERVICES_CONFIG_PATH
                  value: "/workspace/ml_infra_core/llm_engine.core/llm_engine.core/configs/config.yaml"
                - name: CELERY_QUEUE
                  value: "${QUEUE}"
                - name: CELERY_TASK_VISIBILITY
                  value: "VISIBILITY_24H"
                - name: S3_BUCKET
                  value: "${CELERY_S3_BUCKET}"
              resources:
                requests:
                  cpu: 0.1
                  memory: "100M"
                  ephemeral-storage: "100M"
                limits:
                  cpu: ${FORWARDER_CPUS_LIMIT}
                  memory: ${FORWARDER_MEMORY_LIMIT}
                  ephemeral-storage: ${FORWARDER_STORAGE_LIMIT}
              
              
              volumeMounts:
                - name: config-volume
                  mountPath: /root/.aws/config
                  subPath: config
                - name: user-config
                  mountPath: /workspace/user_config
                  subPath: raw_data
                - name: endpoint-config
                  mountPath: /workspace/endpoint_config
                  subPath: raw_data
                - name: infra-service-config-volume
                  mountPath: /workspace/ml_infra_core/llm_engine.core/llm_engine.core/configs
            - name: tritonserver
              image: 000000000000.dkr.ecr.us-west-2.amazonaws.com/std-ml-srv:${TRITON_COMMIT_TAG}-triton
              imagePullPolicy: IfNotPresent
              command:
                - /usr/bin/dumb-init
                - --
                - bash
                - -c
                - "$TRITON_COMMAND"
              env:
                - name: AWS_PROFILE
                  value: "${AWS_ROLE}"
              ports:
                - containerPort: 8000
                  name: http
                - containerPort: 8001
                  name: grpc
                - containerPort: 8002
                  name: metrics
              readinessProbe:
                httpGet:
                # Need to have Triton support --http-address IPv6 :(
                # https://github:com/triton-inference-server/server/issues/5305:
                #   path: /v2/health/ready
                #   port: 8000
                  path: /readyz
                  port: 3000
                initialDelaySeconds: $TRITON_READINESS_INITIAL_DELAY
                periodSeconds: 10
              resources:
                requests:
                  cpu: ${TRITON_CPUS}
                  ${TRITON_MEMORY_DICT}
                  ${TRITON_STORAGE_DICT}
                limits:
                  cpu: ${TRITON_CPUS}
                  ${TRITON_MEMORY_DICT}
                  ${TRITON_STORAGE_DICT}
              volumeMounts:
                - name: config-volume
                  mountPath: /root/.aws/config
                  subPath: config
                - mountPath: /dev/shm
                  name: dshm
            - name: main
              securityContext:
                capabilities:
                  drop:
                  - all
              image: ${IMAGE}
              imagePullPolicy: IfNotPresent
              command: ${COMMAND}
              env: ${MAIN_ENV}
              readinessProbe:
                httpGet:
                  path: ${HEALTHCHECK_ROUTE}
                  port: ${USER_CONTAINER_PORT}
                initialDelaySeconds: ${READINESS_INITIAL_DELAY}
                periodSeconds: 5
              resources:
                requests:
                  cpu: ${CPUS}
                  memory: ${MEMORY}
                  ${STORAGE_DICT}
                limits:
                  cpu: ${CPUS}
                  memory: ${MEMORY}
                  ${STORAGE_DICT}
              volumeMounts:
                - name: config-volume
                  mountPath: /root/.aws/config
                  subPath: config
                - mountPath: /dev/shm
                  name: dshm
                - name: infra-service-config-volume
                  mountPath: ${INFRA_SERVICE_CONFIG_VOLUME_MOUNT_PATH}
                # LIRA: For compatibility with runnable image converted from artifactlike bundle
                - name: config-volume
                  mountPath: /home/llmengine/.aws/config
                  subPath: config
                - name: user-config
                  mountPath: /app/user_config
                  subPath: raw_data
                - name: endpoint-config
                  mountPath: /app/endpoint_config
                  subPath: raw_data
              ports:
                - containerPort: ${USER_CONTAINER_PORT}
                  name: http
          # Workaround for https://github.com/kubernetes-sigs/external-dns/pull/1185
          securityContext:
            fsGroup: 65534
          volumes:
            - name: config-volume
              configMap:
                name: default-config  
            - name: user-config
              configMap:
                name: ${RESOURCE_NAME}
            - name: endpoint-config
              configMap:
                name: ${RESOURCE_NAME}-endpoint-config
            - name: dshm
              emptyDir:
                medium: Memory
            - name: infra-service-config-volume
              configMap:
                name: llm-engine-service-config
                items:
                  - key: infra_service_config
                    path: config.yaml
  deployment-runnable-image-async-cpu.yaml: |-
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: ${RESOURCE_NAME}
      namespace: ${NAMESPACE}
      labels:
        user_id: ${OWNER}
        team: ${TEAM}
        product: ${PRODUCT}
        created_by: ${CREATED_BY}
        owner: ${OWNER}
        env: circleci
        managed-by: llm-engine
        use_scale_llm_engine_endpoint_network_policy: "true"
        tags.datadoghq.com/env: circleci
        tags.datadoghq.com/version: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
        tags.datadoghq.com/service: ${ENDPOINT_NAME}
        endpoint_id: ${ENDPOINT_ID}
        endpoint_name: ${ENDPOINT_NAME}
      annotations:
        celery.scaleml.autoscaler/queue: ${QUEUE}
        celery.scaleml.autoscaler/broker: ${BROKER_NAME}
        celery.scaleml.autoscaler/taskVisibility: "VISIBILITY_24H"
        celery.scaleml.autoscaler/perWorker: "${PER_WORKER}"
        celery.scaleml.autoscaler/minWorkers: "${MIN_WORKERS}"
        celery.scaleml.autoscaler/maxWorkers: "${MAX_WORKERS}"
    spec:
      strategy:
        type: RollingUpdate
        rollingUpdate:
          maxSurge: 1
          maxUnavailable: 0
      replicas: ${MIN_WORKERS}
      selector:
        matchLabels:
          app: ${RESOURCE_NAME}
          version: v1
      template:
        metadata:
          labels:
            app: ${RESOURCE_NAME}
            user_id: ${OWNER}
            team: ${TEAM}
            product: ${PRODUCT}
            created_by: ${CREATED_BY}
            owner: ${OWNER}
            env: circleci
            managed-by: llm-engine
            use_scale_llm_engine_endpoint_network_policy: "true"
            tags.datadoghq.com/env: circleci
            tags.datadoghq.com/version: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
            tags.datadoghq.com/service: ${ENDPOINT_NAME}
            endpoint_id: ${ENDPOINT_ID}
            endpoint_name: ${ENDPOINT_NAME}
            sidecar.istio.io/inject: "false"  # TODO: switch to scuttle
            version: v1
          annotations:
            ad.datadoghq.com/main.logs: '[{"service": "${ENDPOINT_NAME}", "source": "python"}]'
            kubernetes.io/change-cause: "${CHANGE_CAUSE_MESSAGE}"
        spec:
          affinity:
            podAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 1
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: app
                      operator: In
                      values:
                      - ${RESOURCE_NAME}
                  topologyKey: kubernetes.io/hostname
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: ${IMAGE_HASH}
                      operator: In
                      values:
                      - "True"
                  topologyKey: kubernetes.io/hostname
          terminationGracePeriodSeconds: 600
          serviceAccount: default
          nodeSelector:
            node-lifecycle: normal
          priorityClassName: ${PRIORITY}
          containers:
            - name: celery-forwarder
              image: 000000000000.dkr.ecr.us-west-2.amazonaws.com/llm-engine:${FORWARDER_IMAGE_TAG}
              imagePullPolicy: IfNotPresent
              command:
                - /usr/bin/dumb-init
                - --
                - ddtrace-run
                - run-service
                - --config
                - /workspace/server/llm_engine_server/inference/configs/${FORWARDER_CONFIG_FILE_NAME}
                - --queue
                - "${QUEUE}"
                - --task-visibility
                - "VISIBILITY_24H"
                - --set
                - "forwarder.model.args.predict_route=${PREDICT_ROUTE}"
                - --set
                - "forwarder.model.args.healthcheck_route=${HEALTHCHECK_ROUTE}"
                - --concurrency
                - "${PER_WORKER}"
              env:
                - name: DATADOG_TRACE_ENABLED
                  value: "${DATADOG_TRACE_ENABLED}"
                - name: DD_SERVICE
                  value: "${ENDPOINT_NAME}"
                - name: DD_ENV
                  value: circleci
                - name: DD_VERSION
                  value: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
                - name: DD_AGENT_HOST
                  valueFrom:
                    fieldRef:
                      fieldPath: status.hostIP
                - name: AWS_PROFILE
                  value: "${AWS_ROLE}"
                - name: RESULTS_S3_BUCKET
                  value: "${RESULTS_S3_BUCKET}"
                - name: BASE_PATH
                  value: "/workspace"
                - name: ML_INFRA_SERVICES_CONFIG_PATH
                  value: "/workspace/ml_infra_core/llm_engine.core/llm_engine.core/configs/config.yaml"
                - name: CELERY_QUEUE
                  value: "${QUEUE}"
                - name: CELERY_TASK_VISIBILITY
                  value: "VISIBILITY_24H"
                - name: S3_BUCKET
                  value: "${CELERY_S3_BUCKET}"
              resources:
                requests:
                  cpu: 0.1
                  memory: "100M"
                  ephemeral-storage: "100M"
                limits:
                  cpu: ${FORWARDER_CPUS_LIMIT}
                  memory: ${FORWARDER_MEMORY_LIMIT}
                  ephemeral-storage: ${FORWARDER_STORAGE_LIMIT}
              
              
              volumeMounts:
                - name: config-volume
                  mountPath: /root/.aws/config
                  subPath: config
                - name: user-config
                  mountPath: /workspace/user_config
                  subPath: raw_data
                - name: endpoint-config
                  mountPath: /workspace/endpoint_config
                  subPath: raw_data
                - name: infra-service-config-volume
                  mountPath: /workspace/ml_infra_core/llm_engine.core/llm_engine.core/configs
            - name: main
              securityContext:
                capabilities:
                  drop:
                  - all
              image: ${IMAGE}
              imagePullPolicy: IfNotPresent
              command: ${COMMAND}
              env: ${MAIN_ENV}
              readinessProbe:
                httpGet:
                  path: ${HEALTHCHECK_ROUTE}
                  port: ${USER_CONTAINER_PORT}
                initialDelaySeconds: ${READINESS_INITIAL_DELAY}
                periodSeconds: 5
              resources:
                requests:
                  cpu: ${CPUS}
                  memory: ${MEMORY}
                  ${STORAGE_DICT}
                limits:
                  cpu: ${CPUS}
                  memory: ${MEMORY}
                  ${STORAGE_DICT}
              volumeMounts:
                - name: config-volume
                  mountPath: /root/.aws/config
                  subPath: config
                - mountPath: /dev/shm
                  name: dshm
                - name: infra-service-config-volume
                  mountPath: ${INFRA_SERVICE_CONFIG_VOLUME_MOUNT_PATH}
                # LIRA: For compatibility with runnable image converted from artifactlike bundle
                - name: config-volume
                  mountPath: /home/llmengine/.aws/config
                  subPath: config
                - name: user-config
                  mountPath: /app/user_config
                  subPath: raw_data
                - name: endpoint-config
                  mountPath: /app/endpoint_config
                  subPath: raw_data
              ports:
                - containerPort: ${USER_CONTAINER_PORT}
                  name: http
          # Workaround for https://github.com/kubernetes-sigs/external-dns/pull/1185
          securityContext:
            fsGroup: 65534
          volumes:
            - name: config-volume
              configMap:
                name: default-config  
            - name: user-config
              configMap:
                name: ${RESOURCE_NAME}
            - name: endpoint-config
              configMap:
                name: ${RESOURCE_NAME}-endpoint-config
            - name: dshm
              emptyDir:
                medium: Memory
            - name: infra-service-config-volume
              configMap:
                name: llm-engine-service-config
                items:
                  - key: infra_service_config
                    path: config.yaml
  deployment-artifact-async-cpu.yaml: |-
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: ${RESOURCE_NAME}
      namespace: ${NAMESPACE}
      labels:
        user_id: ${OWNER}
        team: ${TEAM}
        product: ${PRODUCT}
        created_by: ${CREATED_BY}
        owner: ${OWNER}
        env: circleci
        managed-by: llm-engine
        use_scale_llm_engine_endpoint_network_policy: "true"
        tags.datadoghq.com/env: circleci
        tags.datadoghq.com/version: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
        tags.datadoghq.com/service: ${ENDPOINT_NAME}
        endpoint_id: ${ENDPOINT_ID}
        endpoint_name: ${ENDPOINT_NAME}
      annotations:
        celery.scaleml.autoscaler/queue: ${QUEUE}
        celery.scaleml.autoscaler/broker: ${BROKER_NAME}
        celery.scaleml.autoscaler/taskVisibility: "VISIBILITY_24H"
        celery.scaleml.autoscaler/perWorker: "${PER_WORKER}"
        celery.scaleml.autoscaler/minWorkers: "${MIN_WORKERS}"
        celery.scaleml.autoscaler/maxWorkers: "${MAX_WORKERS}"
    spec:
      strategy:
        type: RollingUpdate
        rollingUpdate:
          maxSurge: 1
          maxUnavailable: 0
      replicas: ${MIN_WORKERS}
      selector:
        matchLabels:
          app: ${RESOURCE_NAME}
          version: v1
      template:
        metadata:
          labels:
            app: ${RESOURCE_NAME}
            user_id: ${OWNER}
            team: ${TEAM}
            product: ${PRODUCT}
            created_by: ${CREATED_BY}
            owner: ${OWNER}
            env: circleci
            managed-by: llm-engine
            use_scale_llm_engine_endpoint_network_policy: "true"
            tags.datadoghq.com/env: circleci
            tags.datadoghq.com/version: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
            tags.datadoghq.com/service: ${ENDPOINT_NAME}
            endpoint_id: ${ENDPOINT_ID}
            endpoint_name: ${ENDPOINT_NAME}
            sidecar.istio.io/inject: "false"  # TODO: switch to scuttle
            version: v1
          annotations:
            ad.datadoghq.com/main.logs: '[{"service": "${ENDPOINT_NAME}", "source": "python"}]'
            kubernetes.io/change-cause: "${CHANGE_CAUSE_MESSAGE}"
        spec:
          affinity:
            podAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 1
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: app
                      operator: In
                      values:
                      - ${RESOURCE_NAME}
                  topologyKey: kubernetes.io/hostname
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: ${IMAGE_HASH}
                      operator: In
                      values:
                      - "True"
                  topologyKey: kubernetes.io/hostname
          terminationGracePeriodSeconds: 600
          serviceAccount: default
          nodeSelector:
            node-lifecycle: normal
          priorityClassName: ${PRIORITY}
          containers:
            - image: ${IMAGE}
              imagePullPolicy: IfNotPresent
              name: main
              securityContext:
                capabilities:
                  drop:
                  - all
              env:
                - name: DATADOG_TRACE_ENABLED
                  value: "${DATADOG_TRACE_ENABLED}"
                - name: DD_SERVICE
                  value: "${ENDPOINT_NAME}"
                - name: DD_ENV
                  value: circleci
                - name: DD_VERSION
                  value: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
                - name: DD_AGENT_HOST
                  valueFrom:
                    fieldRef:
                      fieldPath: status.hostIP
                - name: OMP_NUM_THREADS
                  value: "1"
                - name: BASE_PATH
                  value: "${BASE_PATH}"
                - name: BUNDLE_URL
                  value: "${BUNDLE_URL}"
                - name: LOAD_PREDICT_FN_MODULE_PATH
                  value: "${LOAD_PREDICT_FN_MODULE_PATH}"
                - name: LOAD_MODEL_FN_MODULE_PATH
                  value: "${LOAD_MODEL_FN_MODULE_PATH}"
                - name: AWS_PROFILE
                  value: "${AWS_ROLE}"
                - name: RESULTS_S3_BUCKET
                  value: "${RESULTS_S3_BUCKET}"
                - name: CHILD_FN_INFO
                  value: "${CHILD_FN_INFO}"
                - name: PREWARM
                  value: "${PREWARM}"
                - name: ML_INFRA_SERVICES_CONFIG_PATH
                  value: "${BASE_PATH}/ml_infra_core/llm_engine.core/llm_engine.core/configs/config.yaml"
                - name: CELERY_S3_BUCKET
                  value: "${CELERY_S3_BUCKET}"
                - name: BROKER_TYPE
                  value: "${BROKER_TYPE}"
                - name: SQS_PROFILE
                  value: "${SQS_PROFILE}"
                - name: SQS_QUEUE_NAME
                  value: "${QUEUE}"
                - name: SQS_QUEUE_URL
                  value: "${SQS_QUEUE_URL}"
              readinessProbe:
                exec:
                  command:
                    - cat
                    - /tmp/readyz
                initialDelaySeconds: 2
                periodSeconds: 2
                failureThreshold: 100
              command: [ "dumb-init", "--", "ddtrace-run" ]
              # Not including --pool=solo means there's a worker process and a separate supervisor process
              # meaning if the worker crashes (because of OOM or something) the supervisor process can mark the task as
              # failed, which should get rid of infinite task retries
              args:
                - celery
                - --app=llm_engine.inference.async_inference
                - worker
                - --loglevel=INFO
                - --concurrency=1
                - --queues=${QUEUE}
                - -O
                - fair
              resources:
                requests:
                  cpu: ${CPUS}
                  memory: ${MEMORY}
                  ${STORAGE_DICT}
                limits:
                  cpu: ${CPUS}
                  memory: ${MEMORY}
                  ${STORAGE_DICT}
              volumeMounts:
                - name: config-volume
                  mountPath: /root/.aws/config
                  subPath: config
                - name: config-volume
                  mountPath: /home/llmengine/.aws/config
                  subPath: config
                - name: user-config
                  mountPath: ${BASE_PATH}/user_config
                  subPath: raw_data
                - name: endpoint-config
                  mountPath: ${BASE_PATH}/endpoint_config
                  subPath: raw_data
                - name: infra-service-config-volume
                  mountPath: ${BASE_PATH}/ml_infra_core/llm_engine.core/llm_engine.core/configs
          # Workaround for https://github.com/kubernetes-sigs/external-dns/pull/1185
          securityContext:
            fsGroup: 65534
          volumes:
            - name: config-volume
              configMap:
                name: default-config  
            - name: user-config
              configMap:
                name: ${RESOURCE_NAME}
            - name: endpoint-config
              configMap:
                name: ${RESOURCE_NAME}-endpoint-config
            - name: dshm
              emptyDir:
                medium: Memory
            - name: infra-service-config-volume
              configMap:
                name: llm-engine-service-config
                items:
                  - key: infra_service_config
                    path: config.yaml
  deployment-triton-enhanced-runnable-image-sync-cpu.yaml: |-
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: ${RESOURCE_NAME}
      namespace: ${NAMESPACE}
      labels:
        user_id: ${OWNER}
        team: ${TEAM}
        product: ${PRODUCT}
        created_by: ${CREATED_BY}
        owner: ${OWNER}
        env: circleci
        managed-by: llm-engine
        use_scale_llm_engine_endpoint_network_policy: "true"
        tags.datadoghq.com/env: circleci
        tags.datadoghq.com/version: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
        tags.datadoghq.com/service: ${ENDPOINT_NAME}
        endpoint_id: ${ENDPOINT_ID}
        endpoint_name: ${ENDPOINT_NAME}
    spec:
      strategy:
        type: RollingUpdate
        rollingUpdate:
          maxSurge: 1
          maxUnavailable: 0
      replicas: ${MIN_WORKERS}
      selector:
        matchLabels:
          app: ${RESOURCE_NAME}
          version: v1
      template:
        metadata:
          labels:
            app: ${RESOURCE_NAME}
            user_id: ${OWNER}
            team: ${TEAM}
            product: ${PRODUCT}
            created_by: ${CREATED_BY}
            owner: ${OWNER}
            env: circleci
            managed-by: llm-engine
            use_scale_llm_engine_endpoint_network_policy: "true"
            tags.datadoghq.com/env: circleci
            tags.datadoghq.com/version: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
            tags.datadoghq.com/service: ${ENDPOINT_NAME}
            endpoint_id: ${ENDPOINT_ID}
            endpoint_name: ${ENDPOINT_NAME}
            version: v1
          annotations:
            ad.datadoghq.com/main.logs: '[{"service": "${ENDPOINT_NAME}", "source": "python"}]'
            kubernetes.io/change-cause: "${CHANGE_CAUSE_MESSAGE}"
        spec:
          affinity:
            podAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 1
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: app
                      operator: In
                      values:
                      - ${RESOURCE_NAME}
                  topologyKey: kubernetes.io/hostname
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: ${IMAGE_HASH}
                      operator: In
                      values:
                      - "True"
                  topologyKey: kubernetes.io/hostname
          terminationGracePeriodSeconds: 600
          serviceAccount: default
          nodeSelector:
            node-lifecycle: normal
          priorityClassName: ${PRIORITY}
          containers:
            - name: http-forwarder
              image: 000000000000.dkr.ecr.us-west-2.amazonaws.com/llm-engine:${FORWARDER_IMAGE_TAG}
              imagePullPolicy: IfNotPresent
              command:
                - /usr/bin/dumb-init
                - --
                - ddtrace-run
                - run-service
                - --config
                - /workspace/server/llm_engine_server/inference/configs/${FORWARDER_CONFIG_FILE_NAME}
                - --http
                - production_threads
                - --port
                - "${FORWARDER_PORT}"
                - --concurrency
                - "${PER_WORKER}"
                - --set
                - "forwarder.model.args.predict_route=${PREDICT_ROUTE}"
                - --set
                - "forwarder.model.args.healthcheck_route=${HEALTHCHECK_ROUTE}"
              env:
                - name: DATADOG_TRACE_ENABLED
                  value: "${DATADOG_TRACE_ENABLED}"
                - name: DD_SERVICE
                  value: "${ENDPOINT_NAME}"
                - name: DD_ENV
                  value: circleci
                - name: DD_VERSION
                  value: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
                - name: DD_AGENT_HOST
                  valueFrom:
                    fieldRef:
                      fieldPath: status.hostIP
                - name: AWS_PROFILE
                  value: "${AWS_ROLE}"
                - name: RESULTS_S3_BUCKET
                  value: "${RESULTS_S3_BUCKET}"
                - name: BASE_PATH
                  value: "/workspace"
                - name: ML_INFRA_SERVICES_CONFIG_PATH
                  value: "/workspace/ml_infra_core/llm_engine.core/llm_engine.core/configs/config.yaml"
                - name: HTTP_HOST
                  value: "0.0.0.0"
              readinessProbe:
                httpGet:
                  path: /readyz
                  port: ${FORWARDER_PORT}
                initialDelaySeconds: ${READINESS_INITIAL_DELAY}
                periodSeconds: 5
              resources:
                requests:
                  cpu: 0.1
                  memory: "100M"
                  ephemeral-storage: "100M"
                limits:
                  cpu: ${FORWARDER_CPUS_LIMIT}
                  memory: ${FORWARDER_MEMORY_LIMIT}
                  ephemeral-storage: ${FORWARDER_STORAGE_LIMIT}
              
              
              volumeMounts:
                - name: config-volume
                  mountPath: /root/.aws/config
                  subPath: config
                - name: user-config
                  mountPath: /workspace/user_config
                  subPath: raw_data
                - name: endpoint-config
                  mountPath: /workspace/endpoint_config
                  subPath: raw_data
                - name: infra-service-config-volume
                  mountPath: /workspace/ml_infra_core/llm_engine.core/llm_engine.core/configs
              ports:
                - containerPort: ${FORWARDER_PORT}
                  name: http
            - name: tritonserver
              image: 000000000000.dkr.ecr.us-west-2.amazonaws.com/std-ml-srv:${TRITON_COMMIT_TAG}-triton
              imagePullPolicy: IfNotPresent
              command:
                - /usr/bin/dumb-init
                - --
                - bash
                - -c
                - "$TRITON_COMMAND"
              env:
                - name: AWS_PROFILE
                  value: "${AWS_ROLE}"
              ports:
                - containerPort: 8000
                  name: http
                - containerPort: 8001
                  name: grpc
                - containerPort: 8002
                  name: metrics
              readinessProbe:
                httpGet:
                # Need to have Triton support --http-address IPv6 :(
                # https://github:com/triton-inference-server/server/issues/5305:
                #   path: /v2/health/ready
                #   port: 8000
                  path: /readyz
                  port: 3000
                initialDelaySeconds: $TRITON_READINESS_INITIAL_DELAY
                periodSeconds: 10
              resources:
                requests:
                  cpu: ${TRITON_CPUS}
                  ${TRITON_MEMORY_DICT}
                  ${TRITON_STORAGE_DICT}
                limits:
                  cpu: ${TRITON_CPUS}
                  ${TRITON_MEMORY_DICT}
                  ${TRITON_STORAGE_DICT}
              volumeMounts:
                - name: config-volume
                  mountPath: /root/.aws/config
                  subPath: config
                - mountPath: /dev/shm
                  name: dshm
            - name: main
              securityContext:
                capabilities:
                  drop:
                  - all
              image: ${IMAGE}
              imagePullPolicy: IfNotPresent
              command: ${COMMAND}
              env: ${MAIN_ENV}
              readinessProbe:
                httpGet:
                  path: ${HEALTHCHECK_ROUTE}
                  port: ${USER_CONTAINER_PORT}
                initialDelaySeconds: ${READINESS_INITIAL_DELAY}
                periodSeconds: 5
              resources:
                requests:
                  cpu: ${CPUS}
                  memory: ${MEMORY}
                  ${STORAGE_DICT}
                limits:
                  cpu: ${CPUS}
                  memory: ${MEMORY}
                  ${STORAGE_DICT}
              volumeMounts:
                - name: config-volume
                  mountPath: /root/.aws/config
                  subPath: config
                - mountPath: /dev/shm
                  name: dshm
                - name: infra-service-config-volume
                  mountPath: ${INFRA_SERVICE_CONFIG_VOLUME_MOUNT_PATH}
                # LIRA: For compatibility with runnable image converted from artifactlike bundle
                - name: config-volume
                  mountPath: /home/llmengine/.aws/config
                  subPath: config
                - name: user-config
                  mountPath: /app/user_config
                  subPath: raw_data
                - name: endpoint-config
                  mountPath: /app/endpoint_config
                  subPath: raw_data
              ports:
                - containerPort: ${USER_CONTAINER_PORT}
                  name: http
          # Workaround for https://github.com/kubernetes-sigs/external-dns/pull/1185
          securityContext:
            fsGroup: 65534
          volumes:
            - name: config-volume
              configMap:
                name: default-config  
            - name: user-config
              configMap:
                name: ${RESOURCE_NAME}
            - name: endpoint-config
              configMap:
                name: ${RESOURCE_NAME}-endpoint-config
            - name: dshm
              emptyDir:
                medium: Memory
            - name: infra-service-config-volume
              configMap:
                name: llm-engine-service-config
                items:
                  - key: infra_service_config
                    path: config.yaml
  deployment-runnable-image-sync-cpu.yaml: |-
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: ${RESOURCE_NAME}
      namespace: ${NAMESPACE}
      labels:
        user_id: ${OWNER}
        team: ${TEAM}
        product: ${PRODUCT}
        created_by: ${CREATED_BY}
        owner: ${OWNER}
        env: circleci
        managed-by: llm-engine
        use_scale_llm_engine_endpoint_network_policy: "true"
        tags.datadoghq.com/env: circleci
        tags.datadoghq.com/version: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
        tags.datadoghq.com/service: ${ENDPOINT_NAME}
        endpoint_id: ${ENDPOINT_ID}
        endpoint_name: ${ENDPOINT_NAME}
    spec:
      strategy:
        type: RollingUpdate
        rollingUpdate:
          maxSurge: 1
          maxUnavailable: 0
      replicas: ${MIN_WORKERS}
      selector:
        matchLabels:
          app: ${RESOURCE_NAME}
          version: v1
      template:
        metadata:
          labels:
            app: ${RESOURCE_NAME}
            user_id: ${OWNER}
            team: ${TEAM}
            product: ${PRODUCT}
            created_by: ${CREATED_BY}
            owner: ${OWNER}
            env: circleci
            managed-by: llm-engine
            use_scale_llm_engine_endpoint_network_policy: "true"
            tags.datadoghq.com/env: circleci
            tags.datadoghq.com/version: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
            tags.datadoghq.com/service: ${ENDPOINT_NAME}
            endpoint_id: ${ENDPOINT_ID}
            endpoint_name: ${ENDPOINT_NAME}
            version: v1
          annotations:
            ad.datadoghq.com/main.logs: '[{"service": "${ENDPOINT_NAME}", "source": "python"}]'
            kubernetes.io/change-cause: "${CHANGE_CAUSE_MESSAGE}"
        spec:
          affinity:
            podAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 1
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: app
                      operator: In
                      values:
                      - ${RESOURCE_NAME}
                  topologyKey: kubernetes.io/hostname
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: ${IMAGE_HASH}
                      operator: In
                      values:
                      - "True"
                  topologyKey: kubernetes.io/hostname
          terminationGracePeriodSeconds: 600
          serviceAccount: default
          nodeSelector:
            node-lifecycle: normal
          priorityClassName: ${PRIORITY}
          containers:
            - name: http-forwarder
              image: 000000000000.dkr.ecr.us-west-2.amazonaws.com/llm-engine:${FORWARDER_IMAGE_TAG}
              imagePullPolicy: IfNotPresent
              command:
                - /usr/bin/dumb-init
                - --
                - ddtrace-run
                - run-service
                - --config
                - /workspace/server/llm_engine_server/inference/configs/${FORWARDER_CONFIG_FILE_NAME}
                - --http
                - production_threads
                - --port
                - "${FORWARDER_PORT}"
                - --concurrency
                - "${PER_WORKER}"
                - --set
                - "forwarder.model.args.predict_route=${PREDICT_ROUTE}"
                - --set
                - "forwarder.model.args.healthcheck_route=${HEALTHCHECK_ROUTE}"
              env:
                - name: DATADOG_TRACE_ENABLED
                  value: "${DATADOG_TRACE_ENABLED}"
                - name: DD_SERVICE
                  value: "${ENDPOINT_NAME}"
                - name: DD_ENV
                  value: circleci
                - name: DD_VERSION
                  value: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
                - name: DD_AGENT_HOST
                  valueFrom:
                    fieldRef:
                      fieldPath: status.hostIP
                - name: AWS_PROFILE
                  value: "${AWS_ROLE}"
                - name: RESULTS_S3_BUCKET
                  value: "${RESULTS_S3_BUCKET}"
                - name: BASE_PATH
                  value: "/workspace"
                - name: ML_INFRA_SERVICES_CONFIG_PATH
                  value: "/workspace/ml_infra_core/llm_engine.core/llm_engine.core/configs/config.yaml"
                - name: HTTP_HOST
                  value: "0.0.0.0"
              readinessProbe:
                httpGet:
                  path: /readyz
                  port: ${FORWARDER_PORT}
                initialDelaySeconds: ${READINESS_INITIAL_DELAY}
                periodSeconds: 5
              resources:
                requests:
                  cpu: 0.1
                  memory: "100M"
                  ephemeral-storage: "100M"
                limits:
                  cpu: ${FORWARDER_CPUS_LIMIT}
                  memory: ${FORWARDER_MEMORY_LIMIT}
                  ephemeral-storage: ${FORWARDER_STORAGE_LIMIT}
              
              
              volumeMounts:
                - name: config-volume
                  mountPath: /root/.aws/config
                  subPath: config
                - name: user-config
                  mountPath: /workspace/user_config
                  subPath: raw_data
                - name: endpoint-config
                  mountPath: /workspace/endpoint_config
                  subPath: raw_data
                - name: infra-service-config-volume
                  mountPath: /workspace/ml_infra_core/llm_engine.core/llm_engine.core/configs
              ports:
                - containerPort: ${FORWARDER_PORT}
                  name: http
            - name: main
              securityContext:
                capabilities:
                  drop:
                  - all
              image: ${IMAGE}
              imagePullPolicy: IfNotPresent
              command: ${COMMAND}
              env: ${MAIN_ENV}
              readinessProbe:
                httpGet:
                  path: ${HEALTHCHECK_ROUTE}
                  port: ${USER_CONTAINER_PORT}
                initialDelaySeconds: ${READINESS_INITIAL_DELAY}
                periodSeconds: 5
              resources:
                requests:
                  cpu: ${CPUS}
                  memory: ${MEMORY}
                  ${STORAGE_DICT}
                limits:
                  cpu: ${CPUS}
                  memory: ${MEMORY}
                  ${STORAGE_DICT}
              volumeMounts:
                - name: config-volume
                  mountPath: /root/.aws/config
                  subPath: config
                - mountPath: /dev/shm
                  name: dshm
                - name: infra-service-config-volume
                  mountPath: ${INFRA_SERVICE_CONFIG_VOLUME_MOUNT_PATH}
                # LIRA: For compatibility with runnable image converted from artifactlike bundle
                - name: config-volume
                  mountPath: /home/llmengine/.aws/config
                  subPath: config
                - name: user-config
                  mountPath: /app/user_config
                  subPath: raw_data
                - name: endpoint-config
                  mountPath: /app/endpoint_config
                  subPath: raw_data
              ports:
                - containerPort: ${USER_CONTAINER_PORT}
                  name: http
          # Workaround for https://github.com/kubernetes-sigs/external-dns/pull/1185
          securityContext:
            fsGroup: 65534
          volumes:
            - name: config-volume
              configMap:
                name: default-config  
            - name: user-config
              configMap:
                name: ${RESOURCE_NAME}
            - name: endpoint-config
              configMap:
                name: ${RESOURCE_NAME}-endpoint-config
            - name: dshm
              emptyDir:
                medium: Memory
            - name: infra-service-config-volume
              configMap:
                name: llm-engine-service-config
                items:
                  - key: infra_service_config
                    path: config.yaml
  deployment-artifact-sync-cpu.yaml: |-
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: ${RESOURCE_NAME}
      namespace: ${NAMESPACE}
      labels:
        user_id: ${OWNER}
        team: ${TEAM}
        product: ${PRODUCT}
        created_by: ${CREATED_BY}
        owner: ${OWNER}
        env: circleci
        managed-by: llm-engine
        use_scale_llm_engine_endpoint_network_policy: "true"
        tags.datadoghq.com/env: circleci
        tags.datadoghq.com/version: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
        tags.datadoghq.com/service: ${ENDPOINT_NAME}
        endpoint_id: ${ENDPOINT_ID}
        endpoint_name: ${ENDPOINT_NAME}
    spec:
      strategy:
        type: RollingUpdate
        rollingUpdate:
          maxSurge: 1
          maxUnavailable: 0
      replicas: ${MIN_WORKERS}
      selector:
        matchLabels:
          app: ${RESOURCE_NAME}
          version: v1
      template:
        metadata:
          labels:
            app: ${RESOURCE_NAME}
            user_id: ${OWNER}
            team: ${TEAM}
            product: ${PRODUCT}
            created_by: ${CREATED_BY}
            owner: ${OWNER}
            env: circleci
            managed-by: llm-engine
            use_scale_llm_engine_endpoint_network_policy: "true"
            tags.datadoghq.com/env: circleci
            tags.datadoghq.com/version: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
            tags.datadoghq.com/service: ${ENDPOINT_NAME}
            endpoint_id: ${ENDPOINT_ID}
            endpoint_name: ${ENDPOINT_NAME}
            version: v1
          annotations:
            ad.datadoghq.com/main.logs: '[{"service": "${ENDPOINT_NAME}", "source": "python"}]'
            kubernetes.io/change-cause: "${CHANGE_CAUSE_MESSAGE}"
        spec:
          affinity:
            podAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 1
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: app
                      operator: In
                      values:
                      - ${RESOURCE_NAME}
                  topologyKey: kubernetes.io/hostname
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: ${IMAGE_HASH}
                      operator: In
                      values:
                      - "True"
                  topologyKey: kubernetes.io/hostname
          terminationGracePeriodSeconds: 600
          serviceAccount: default
          nodeSelector:
            node-lifecycle: normal
          priorityClassName: ${PRIORITY}
          containers:
            - image: ${IMAGE}
              imagePullPolicy: IfNotPresent
              name: main
              securityContext:
                capabilities:
                  drop:
                  - all
              env:
                - name: DATADOG_TRACE_ENABLED
                  value: "${DATADOG_TRACE_ENABLED}"
                - name: DD_SERVICE
                  value: "${ENDPOINT_NAME}"
                - name: DD_ENV
                  value: circleci
                - name: DD_VERSION
                  value: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
                - name: DD_AGENT_HOST
                  valueFrom:
                    fieldRef:
                      fieldPath: status.hostIP
                - name: OMP_NUM_THREADS
                  value: "1"
                - name: BASE_PATH
                  value: "${BASE_PATH}"
                - name: BUNDLE_URL
                  value: "${BUNDLE_URL}"
                - name: LOAD_PREDICT_FN_MODULE_PATH
                  value: "${LOAD_PREDICT_FN_MODULE_PATH}"
                - name: LOAD_MODEL_FN_MODULE_PATH
                  value: "${LOAD_MODEL_FN_MODULE_PATH}"
                - name: AWS_PROFILE
                  value: "${AWS_ROLE}"
                - name: RESULTS_S3_BUCKET
                  value: "${RESULTS_S3_BUCKET}"
                - name: CHILD_FN_INFO
                  value: "${CHILD_FN_INFO}"
                - name: PREWARM
                  value: "${PREWARM}"
                - name: ML_INFRA_SERVICES_CONFIG_PATH
                  value: "${BASE_PATH}/ml_infra_core/llm_engine.core/llm_engine.core/configs/config.yaml"
                - name: PORT
                  value: "${ARTIFACT_LIKE_CONTAINER_PORT}"
              readinessProbe:
                httpGet:
                  path: /readyz
                  port: ${ARTIFACT_LIKE_CONTAINER_PORT}
                initialDelaySeconds: 2
                periodSeconds: 2
                failureThreshold: 100
              command: [ "dumb-init", "--", "ddtrace-run" ]
              args:
                - python
                - -m
                - llm_engine.inference.sync_inference.start_fastapi_server
              resources:
                requests:
                  cpu: ${CPUS}
                  memory: ${MEMORY}
                  ${STORAGE_DICT}
                limits:
                  cpu: ${CPUS}
                  memory: ${MEMORY}
                  ${STORAGE_DICT}
              volumeMounts:
                - name: config-volume
                  mountPath: /root/.aws/config
                  subPath: config
                - name: config-volume
                  mountPath: /home/llmengine/.aws/config
                  subPath: config
                - name: user-config
                  mountPath: ${BASE_PATH}/user_config
                  subPath: raw_data
                - name: endpoint-config
                  mountPath: ${BASE_PATH}/endpoint_config
                  subPath: raw_data
                - name: infra-service-config-volume
                  mountPath: ${BASE_PATH}/ml_infra_core/llm_engine.core/llm_engine.core/configs
          # Workaround for https://github.com/kubernetes-sigs/external-dns/pull/1185
          securityContext:
            fsGroup: 65534
          volumes:
            - name: config-volume
              configMap:
                name: default-config  
            - name: user-config
              configMap:
                name: ${RESOURCE_NAME}
            - name: endpoint-config
              configMap:
                name: ${RESOURCE_NAME}-endpoint-config
            - name: dshm
              emptyDir:
                medium: Memory
            - name: infra-service-config-volume
              configMap:
                name: llm-engine-service-config
                items:
                  - key: infra_service_config
                    path: config.yaml
  deployment-runnable-image-streaming-cpu.yaml: |-
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: ${RESOURCE_NAME}
      namespace: ${NAMESPACE}
      labels:
        user_id: ${OWNER}
        team: ${TEAM}
        product: ${PRODUCT}
        created_by: ${CREATED_BY}
        owner: ${OWNER}
        env: circleci
        managed-by: llm-engine
        use_scale_llm_engine_endpoint_network_policy: "true"
        tags.datadoghq.com/env: circleci
        tags.datadoghq.com/version: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
        tags.datadoghq.com/service: ${ENDPOINT_NAME}
        endpoint_id: ${ENDPOINT_ID}
        endpoint_name: ${ENDPOINT_NAME}
    spec:
      strategy:
        type: RollingUpdate
        rollingUpdate:
          maxSurge: 1
          maxUnavailable: 0
      replicas: ${MIN_WORKERS}
      selector:
        matchLabels:
          app: ${RESOURCE_NAME}
          version: v1
      template:
        metadata:
          labels:
            app: ${RESOURCE_NAME}
            user_id: ${OWNER}
            team: ${TEAM}
            product: ${PRODUCT}
            created_by: ${CREATED_BY}
            owner: ${OWNER}
            env: circleci
            managed-by: llm-engine
            use_scale_llm_engine_endpoint_network_policy: "true"
            tags.datadoghq.com/env: circleci
            tags.datadoghq.com/version: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
            tags.datadoghq.com/service: ${ENDPOINT_NAME}
            endpoint_id: ${ENDPOINT_ID}
            endpoint_name: ${ENDPOINT_NAME}
            version: v1
          annotations:
            ad.datadoghq.com/main.logs: '[{"service": "${ENDPOINT_NAME}", "source": "python"}]'
            kubernetes.io/change-cause: "${CHANGE_CAUSE_MESSAGE}"
        spec:
          affinity:
            podAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 1
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: app
                      operator: In
                      values:
                      - ${RESOURCE_NAME}
                  topologyKey: kubernetes.io/hostname
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: ${IMAGE_HASH}
                      operator: In
                      values:
                      - "True"
                  topologyKey: kubernetes.io/hostname
          terminationGracePeriodSeconds: 600
          serviceAccount: default
          nodeSelector:
            node-lifecycle: normal
          priorityClassName: ${PRIORITY}
          containers:
            - name: http-forwarder
              image: 000000000000.dkr.ecr.us-west-2.amazonaws.com/llm-engine:${FORWARDER_IMAGE_TAG}
              imagePullPolicy: IfNotPresent
              command:
                - /usr/bin/dumb-init
                - --
                - ddtrace-run
                - python
                - -m
                - server.llm_engine_server.inference.forwarding.http_forwarder
                - --config
                - /workspace/server/llm_engine_server/inference/configs/service--http_forwarder.yaml
                - --port
                - "${FORWARDER_PORT}"
                - --num-workers
                - "${PER_WORKER}"
                - --set
                - "forwarder.sync.predict_route=${PREDICT_ROUTE}"
                - --set
                - "forwarder.stream.predict_route=${STREAMING_PREDICT_ROUTE}"
                - --set
                - "forwarder.sync.healthcheck_route=${HEALTHCHECK_ROUTE}"
                - --set
                - "forwarder.stream.healthcheck_route=${HEALTHCHECK_ROUTE}"
              env:
                - name: DATADOG_TRACE_ENABLED
                  value: "${DATADOG_TRACE_ENABLED}"
                - name: DD_SERVICE
                  value: "${ENDPOINT_NAME}"
                - name: DD_ENV
                  value: circleci
                - name: DD_VERSION
                  value: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
                - name: DD_AGENT_HOST
                  valueFrom:
                    fieldRef:
                      fieldPath: status.hostIP
                - name: AWS_PROFILE
                  value: "${AWS_ROLE}"
                - name: RESULTS_S3_BUCKET
                  value: "${RESULTS_S3_BUCKET}"
                - name: BASE_PATH
                  value: "/workspace"
                - name: ML_INFRA_SERVICES_CONFIG_PATH
                  value: "/workspace/ml_infra_core/llm_engine.core/llm_engine.core/configs/config.yaml"
                - name: HTTP_HOST
                  value: "0.0.0.0"
              readinessProbe:
                httpGet:
                  path: /readyz
                  port: ${FORWARDER_PORT}
                initialDelaySeconds: ${READINESS_INITIAL_DELAY}
                periodSeconds: 5
              resources:
                requests:
                  cpu: 0.1
                  memory: "100M"
                  ephemeral-storage: "100M"
                limits:
                  cpu: ${FORWARDER_CPUS_LIMIT}
                  memory: ${FORWARDER_MEMORY_LIMIT}
                  ephemeral-storage: ${FORWARDER_STORAGE_LIMIT}
              
              
              volumeMounts:
                - name: config-volume
                  mountPath: /root/.aws/config
                  subPath: config
                - name: user-config
                  mountPath: /workspace/user_config
                  subPath: raw_data
                - name: endpoint-config
                  mountPath: /workspace/endpoint_config
                  subPath: raw_data
                - name: infra-service-config-volume
                  mountPath: /workspace/ml_infra_core/llm_engine.core/llm_engine.core/configs
              ports:
                - containerPort: ${FORWARDER_PORT}
                  name: http
            - name: main
              securityContext:
                capabilities:
                  drop:
                  - all
              image: ${IMAGE}
              imagePullPolicy: IfNotPresent
              command: ${COMMAND}
              env: ${MAIN_ENV}
              readinessProbe:
                httpGet:
                  path: ${HEALTHCHECK_ROUTE}
                  port: ${USER_CONTAINER_PORT}
                initialDelaySeconds: ${READINESS_INITIAL_DELAY}
                periodSeconds: 5
              resources:
                requests:
                  cpu: ${CPUS}
                  memory: ${MEMORY}
                  ${STORAGE_DICT}
                limits:
                  cpu: ${CPUS}
                  memory: ${MEMORY}
                  ${STORAGE_DICT}
              volumeMounts:
                - name: config-volume
                  mountPath: /root/.aws/config
                  subPath: config
                - mountPath: /dev/shm
                  name: dshm
                - name: infra-service-config-volume
                  mountPath: ${INFRA_SERVICE_CONFIG_VOLUME_MOUNT_PATH}
                # LIRA: For compatibility with runnable image converted from artifactlike bundle
                - name: config-volume
                  mountPath: /home/llmengine/.aws/config
                  subPath: config
                - name: user-config
                  mountPath: /app/user_config
                  subPath: raw_data
                - name: endpoint-config
                  mountPath: /app/endpoint_config
                  subPath: raw_data
              ports:
                - containerPort: ${USER_CONTAINER_PORT}
                  name: http
          # Workaround for https://github.com/kubernetes-sigs/external-dns/pull/1185
          securityContext:
            fsGroup: 65534
          volumes:
            - name: config-volume
              configMap:
                name: default-config  
            - name: user-config
              configMap:
                name: ${RESOURCE_NAME}
            - name: endpoint-config
              configMap:
                name: ${RESOURCE_NAME}-endpoint-config
            - name: dshm
              emptyDir:
                medium: Memory
            - name: infra-service-config-volume
              configMap:
                name: llm-engine-service-config
                items:
                  - key: infra_service_config
                    path: config.yaml
  deployment-triton-enhanced-runnable-image-async-gpu.yaml: |-
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: ${RESOURCE_NAME}
      namespace: ${NAMESPACE}
      labels:
        user_id: ${OWNER}
        team: ${TEAM}
        product: ${PRODUCT}
        created_by: ${CREATED_BY}
        owner: ${OWNER}
        env: circleci
        managed-by: llm-engine
        use_scale_llm_engine_endpoint_network_policy: "true"
        tags.datadoghq.com/env: circleci
        tags.datadoghq.com/version: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
        tags.datadoghq.com/service: ${ENDPOINT_NAME}
        endpoint_id: ${ENDPOINT_ID}
        endpoint_name: ${ENDPOINT_NAME}
      annotations:
        celery.scaleml.autoscaler/queue: ${QUEUE}
        celery.scaleml.autoscaler/broker: ${BROKER_NAME}
        celery.scaleml.autoscaler/taskVisibility: "VISIBILITY_24H"
        celery.scaleml.autoscaler/perWorker: "${PER_WORKER}"
        celery.scaleml.autoscaler/minWorkers: "${MIN_WORKERS}"
        celery.scaleml.autoscaler/maxWorkers: "${MAX_WORKERS}"
    spec:
      strategy:
        type: RollingUpdate
        rollingUpdate:
          maxSurge: 1
          maxUnavailable: 0
      replicas: ${MIN_WORKERS}
      selector:
        matchLabels:
          app: ${RESOURCE_NAME}
          version: v1
      template:
        metadata:
          labels:
            app: ${RESOURCE_NAME}
            user_id: ${OWNER}
            team: ${TEAM}
            product: ${PRODUCT}
            created_by: ${CREATED_BY}
            owner: ${OWNER}
            env: circleci
            managed-by: llm-engine
            use_scale_llm_engine_endpoint_network_policy: "true"
            tags.datadoghq.com/env: circleci
            tags.datadoghq.com/version: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
            tags.datadoghq.com/service: ${ENDPOINT_NAME}
            endpoint_id: ${ENDPOINT_ID}
            endpoint_name: ${ENDPOINT_NAME}
            sidecar.istio.io/inject: "false"  # TODO: switch to scuttle
            version: v1
          annotations:
            ad.datadoghq.com/main.logs: '[{"service": "${ENDPOINT_NAME}", "source": "python"}]'
            kubernetes.io/change-cause: "${CHANGE_CAUSE_MESSAGE}"
        spec:
          affinity:
            podAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 1
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: app
                      operator: In
                      values:
                      - ${RESOURCE_NAME}
                  topologyKey: kubernetes.io/hostname
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: ${IMAGE_HASH}
                      operator: In
                      values:
                      - "True"
                  topologyKey: kubernetes.io/hostname
          terminationGracePeriodSeconds: 600
          serviceAccount: default
          nodeSelector:
            node-lifecycle: normal
            k8s.amazonaws.com/accelerator: ${GPU_TYPE}
          tolerations:
            - key: "nvidia.com/gpu"
              operator: "Exists"
              effect: "NoSchedule"
          priorityClassName: ${PRIORITY}
          containers:
            - name: celery-forwarder
              image: 000000000000.dkr.ecr.us-west-2.amazonaws.com/llm-engine:${FORWARDER_IMAGE_TAG}
              imagePullPolicy: IfNotPresent
              command:
                - /usr/bin/dumb-init
                - --
                - ddtrace-run
                - run-service
                - --config
                - /workspace/server/llm_engine_server/inference/configs/${FORWARDER_CONFIG_FILE_NAME}
                - --queue
                - "${QUEUE}"
                - --task-visibility
                - "VISIBILITY_24H"
                - --set
                - "forwarder.model.args.predict_route=${PREDICT_ROUTE}"
                - --set
                - "forwarder.model.args.healthcheck_route=${HEALTHCHECK_ROUTE}"
                - --concurrency
                - "${PER_WORKER}"
              env:
                - name: DATADOG_TRACE_ENABLED
                  value: "${DATADOG_TRACE_ENABLED}"
                - name: DD_SERVICE
                  value: "${ENDPOINT_NAME}"
                - name: DD_ENV
                  value: circleci
                - name: DD_VERSION
                  value: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
                - name: DD_AGENT_HOST
                  valueFrom:
                    fieldRef:
                      fieldPath: status.hostIP
                - name: AWS_PROFILE
                  value: "${AWS_ROLE}"
                - name: RESULTS_S3_BUCKET
                  value: "${RESULTS_S3_BUCKET}"
                - name: BASE_PATH
                  value: "/workspace"
                - name: ML_INFRA_SERVICES_CONFIG_PATH
                  value: "/workspace/ml_infra_core/llm_engine.core/llm_engine.core/configs/config.yaml"
                - name: CELERY_QUEUE
                  value: "${QUEUE}"
                - name: CELERY_TASK_VISIBILITY
                  value: "VISIBILITY_24H"
                - name: S3_BUCKET
                  value: "${CELERY_S3_BUCKET}"
              resources:
                requests:
                  cpu: 0.1
                  memory: "100M"
                  ephemeral-storage: "100M"
                limits:
                  cpu: ${FORWARDER_CPUS_LIMIT}
                  memory: ${FORWARDER_MEMORY_LIMIT}
                  ephemeral-storage: ${FORWARDER_STORAGE_LIMIT}
              
              
              volumeMounts:
                - name: config-volume
                  mountPath: /root/.aws/config
                  subPath: config
                - name: user-config
                  mountPath: /workspace/user_config
                  subPath: raw_data
                - name: endpoint-config
                  mountPath: /workspace/endpoint_config
                  subPath: raw_data
                - name: infra-service-config-volume
                  mountPath: /workspace/ml_infra_core/llm_engine.core/llm_engine.core/configs
            - name: tritonserver
              image: 000000000000.dkr.ecr.us-west-2.amazonaws.com/std-ml-srv:${TRITON_COMMIT_TAG}-triton
              imagePullPolicy: IfNotPresent
              command:
                - /usr/bin/dumb-init
                - --
                - bash
                - -c
                - "$TRITON_COMMAND"
              env:
                - name: AWS_PROFILE
                  value: "${AWS_ROLE}"
              ports:
                - containerPort: 8000
                  name: http
                - containerPort: 8001
                  name: grpc
                - containerPort: 8002
                  name: metrics
              readinessProbe:
                httpGet:
                # Need to have Triton support --http-address IPv6 :(
                # https://github:com/triton-inference-server/server/issues/5305:
                #   path: /v2/health/ready
                #   port: 8000
                  path: /readyz
                  port: 3000
                initialDelaySeconds: $TRITON_READINESS_INITIAL_DELAY
                periodSeconds: 10
              resources:
                requests:
                  cpu: ${TRITON_CPUS}
                  ${TRITON_MEMORY_DICT}
                  ${TRITON_STORAGE_DICT}
                limits:
                  cpu: ${TRITON_CPUS}
                  ${TRITON_MEMORY_DICT}
                  ${TRITON_STORAGE_DICT}
              volumeMounts:
                - name: config-volume
                  mountPath: /root/.aws/config
                  subPath: config
                - mountPath: /dev/shm
                  name: dshm
            - name: main
              securityContext:
                capabilities:
                  drop:
                  - all
              image: ${IMAGE}
              imagePullPolicy: IfNotPresent
              command: ${COMMAND}
              env: ${MAIN_ENV}
              readinessProbe:
                httpGet:
                  path: ${HEALTHCHECK_ROUTE}
                  port: ${USER_CONTAINER_PORT}
                initialDelaySeconds: ${READINESS_INITIAL_DELAY}
                periodSeconds: 5
              resources:
                requests:
                  cpu: ${CPUS}
                  memory: ${MEMORY}
                  ${STORAGE_DICT}
                limits:
                  nvidia.com/gpu: ${GPUS}
                  cpu: ${CPUS}
                  memory: ${MEMORY}
                  ${STORAGE_DICT}
              volumeMounts:
                - name: config-volume
                  mountPath: /root/.aws/config
                  subPath: config
                - mountPath: /dev/shm
                  name: dshm
                - name: infra-service-config-volume
                  mountPath: ${INFRA_SERVICE_CONFIG_VOLUME_MOUNT_PATH}
                # LIRA: For compatibility with runnable image converted from artifactlike bundle
                - name: config-volume
                  mountPath: /home/llmengine/.aws/config
                  subPath: config
                - name: user-config
                  mountPath: /app/user_config
                  subPath: raw_data
                - name: endpoint-config
                  mountPath: /app/endpoint_config
                  subPath: raw_data
              ports:
                - containerPort: ${USER_CONTAINER_PORT}
                  name: http
          # Workaround for https://github.com/kubernetes-sigs/external-dns/pull/1185
          securityContext:
            fsGroup: 65534
          volumes:
            - name: config-volume
              configMap:
                name: default-config  
            - name: user-config
              configMap:
                name: ${RESOURCE_NAME}
            - name: endpoint-config
              configMap:
                name: ${RESOURCE_NAME}-endpoint-config
            - name: dshm
              emptyDir:
                medium: Memory
            - name: infra-service-config-volume
              configMap:
                name: llm-engine-service-config
                items:
                  - key: infra_service_config
                    path: config.yaml
  deployment-runnable-image-async-gpu.yaml: |-
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: ${RESOURCE_NAME}
      namespace: ${NAMESPACE}
      labels:
        user_id: ${OWNER}
        team: ${TEAM}
        product: ${PRODUCT}
        created_by: ${CREATED_BY}
        owner: ${OWNER}
        env: circleci
        managed-by: llm-engine
        use_scale_llm_engine_endpoint_network_policy: "true"
        tags.datadoghq.com/env: circleci
        tags.datadoghq.com/version: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
        tags.datadoghq.com/service: ${ENDPOINT_NAME}
        endpoint_id: ${ENDPOINT_ID}
        endpoint_name: ${ENDPOINT_NAME}
      annotations:
        celery.scaleml.autoscaler/queue: ${QUEUE}
        celery.scaleml.autoscaler/broker: ${BROKER_NAME}
        celery.scaleml.autoscaler/taskVisibility: "VISIBILITY_24H"
        celery.scaleml.autoscaler/perWorker: "${PER_WORKER}"
        celery.scaleml.autoscaler/minWorkers: "${MIN_WORKERS}"
        celery.scaleml.autoscaler/maxWorkers: "${MAX_WORKERS}"
    spec:
      strategy:
        type: RollingUpdate
        rollingUpdate:
          maxSurge: 1
          maxUnavailable: 0
      replicas: ${MIN_WORKERS}
      selector:
        matchLabels:
          app: ${RESOURCE_NAME}
          version: v1
      template:
        metadata:
          labels:
            app: ${RESOURCE_NAME}
            user_id: ${OWNER}
            team: ${TEAM}
            product: ${PRODUCT}
            created_by: ${CREATED_BY}
            owner: ${OWNER}
            env: circleci
            managed-by: llm-engine
            use_scale_llm_engine_endpoint_network_policy: "true"
            tags.datadoghq.com/env: circleci
            tags.datadoghq.com/version: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
            tags.datadoghq.com/service: ${ENDPOINT_NAME}
            endpoint_id: ${ENDPOINT_ID}
            endpoint_name: ${ENDPOINT_NAME}
            sidecar.istio.io/inject: "false"  # TODO: switch to scuttle
            version: v1
          annotations:
            ad.datadoghq.com/main.logs: '[{"service": "${ENDPOINT_NAME}", "source": "python"}]'
            kubernetes.io/change-cause: "${CHANGE_CAUSE_MESSAGE}"
        spec:
          affinity:
            podAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 1
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: app
                      operator: In
                      values:
                      - ${RESOURCE_NAME}
                  topologyKey: kubernetes.io/hostname
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: ${IMAGE_HASH}
                      operator: In
                      values:
                      - "True"
                  topologyKey: kubernetes.io/hostname
          terminationGracePeriodSeconds: 600
          serviceAccount: default
          nodeSelector:
            node-lifecycle: normal
            k8s.amazonaws.com/accelerator: ${GPU_TYPE}
          tolerations:
            - key: "nvidia.com/gpu"
              operator: "Exists"
              effect: "NoSchedule"
          priorityClassName: ${PRIORITY}
          containers:
            - name: celery-forwarder
              image: 000000000000.dkr.ecr.us-west-2.amazonaws.com/llm-engine:${FORWARDER_IMAGE_TAG}
              imagePullPolicy: IfNotPresent
              command:
                - /usr/bin/dumb-init
                - --
                - ddtrace-run
                - run-service
                - --config
                - /workspace/server/llm_engine_server/inference/configs/${FORWARDER_CONFIG_FILE_NAME}
                - --queue
                - "${QUEUE}"
                - --task-visibility
                - "VISIBILITY_24H"
                - --set
                - "forwarder.model.args.predict_route=${PREDICT_ROUTE}"
                - --set
                - "forwarder.model.args.healthcheck_route=${HEALTHCHECK_ROUTE}"
                - --concurrency
                - "${PER_WORKER}"
              env:
                - name: DATADOG_TRACE_ENABLED
                  value: "${DATADOG_TRACE_ENABLED}"
                - name: DD_SERVICE
                  value: "${ENDPOINT_NAME}"
                - name: DD_ENV
                  value: circleci
                - name: DD_VERSION
                  value: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
                - name: DD_AGENT_HOST
                  valueFrom:
                    fieldRef:
                      fieldPath: status.hostIP
                - name: AWS_PROFILE
                  value: "${AWS_ROLE}"
                - name: RESULTS_S3_BUCKET
                  value: "${RESULTS_S3_BUCKET}"
                - name: BASE_PATH
                  value: "/workspace"
                - name: ML_INFRA_SERVICES_CONFIG_PATH
                  value: "/workspace/ml_infra_core/llm_engine.core/llm_engine.core/configs/config.yaml"
                - name: CELERY_QUEUE
                  value: "${QUEUE}"
                - name: CELERY_TASK_VISIBILITY
                  value: "VISIBILITY_24H"
                - name: S3_BUCKET
                  value: "${CELERY_S3_BUCKET}"
              resources:
                requests:
                  cpu: 0.1
                  memory: "100M"
                  ephemeral-storage: "100M"
                limits:
                  cpu: ${FORWARDER_CPUS_LIMIT}
                  memory: ${FORWARDER_MEMORY_LIMIT}
                  ephemeral-storage: ${FORWARDER_STORAGE_LIMIT}
              
              
              volumeMounts:
                - name: config-volume
                  mountPath: /root/.aws/config
                  subPath: config
                - name: user-config
                  mountPath: /workspace/user_config
                  subPath: raw_data
                - name: endpoint-config
                  mountPath: /workspace/endpoint_config
                  subPath: raw_data
                - name: infra-service-config-volume
                  mountPath: /workspace/ml_infra_core/llm_engine.core/llm_engine.core/configs
            - name: main
              securityContext:
                capabilities:
                  drop:
                  - all
              image: ${IMAGE}
              imagePullPolicy: IfNotPresent
              command: ${COMMAND}
              env: ${MAIN_ENV}
              readinessProbe:
                httpGet:
                  path: ${HEALTHCHECK_ROUTE}
                  port: ${USER_CONTAINER_PORT}
                initialDelaySeconds: ${READINESS_INITIAL_DELAY}
                periodSeconds: 5
              resources:
                requests:
                  cpu: ${CPUS}
                  memory: ${MEMORY}
                  ${STORAGE_DICT}
                limits:
                  nvidia.com/gpu: ${GPUS}
                  cpu: ${CPUS}
                  memory: ${MEMORY}
                  ${STORAGE_DICT}
              volumeMounts:
                - name: config-volume
                  mountPath: /root/.aws/config
                  subPath: config
                - mountPath: /dev/shm
                  name: dshm
                - name: infra-service-config-volume
                  mountPath: ${INFRA_SERVICE_CONFIG_VOLUME_MOUNT_PATH}
                # LIRA: For compatibility with runnable image converted from artifactlike bundle
                - name: config-volume
                  mountPath: /home/llmengine/.aws/config
                  subPath: config
                - name: user-config
                  mountPath: /app/user_config
                  subPath: raw_data
                - name: endpoint-config
                  mountPath: /app/endpoint_config
                  subPath: raw_data
              ports:
                - containerPort: ${USER_CONTAINER_PORT}
                  name: http
          # Workaround for https://github.com/kubernetes-sigs/external-dns/pull/1185
          securityContext:
            fsGroup: 65534
          volumes:
            - name: config-volume
              configMap:
                name: default-config  
            - name: user-config
              configMap:
                name: ${RESOURCE_NAME}
            - name: endpoint-config
              configMap:
                name: ${RESOURCE_NAME}-endpoint-config
            - name: dshm
              emptyDir:
                medium: Memory
            - name: infra-service-config-volume
              configMap:
                name: llm-engine-service-config
                items:
                  - key: infra_service_config
                    path: config.yaml
  deployment-artifact-async-gpu.yaml: |-
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: ${RESOURCE_NAME}
      namespace: ${NAMESPACE}
      labels:
        user_id: ${OWNER}
        team: ${TEAM}
        product: ${PRODUCT}
        created_by: ${CREATED_BY}
        owner: ${OWNER}
        env: circleci
        managed-by: llm-engine
        use_scale_llm_engine_endpoint_network_policy: "true"
        tags.datadoghq.com/env: circleci
        tags.datadoghq.com/version: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
        tags.datadoghq.com/service: ${ENDPOINT_NAME}
        endpoint_id: ${ENDPOINT_ID}
        endpoint_name: ${ENDPOINT_NAME}
      annotations:
        celery.scaleml.autoscaler/queue: ${QUEUE}
        celery.scaleml.autoscaler/broker: ${BROKER_NAME}
        celery.scaleml.autoscaler/taskVisibility: "VISIBILITY_24H"
        celery.scaleml.autoscaler/perWorker: "${PER_WORKER}"
        celery.scaleml.autoscaler/minWorkers: "${MIN_WORKERS}"
        celery.scaleml.autoscaler/maxWorkers: "${MAX_WORKERS}"
    spec:
      strategy:
        type: RollingUpdate
        rollingUpdate:
          maxSurge: 1
          maxUnavailable: 0
      replicas: ${MIN_WORKERS}
      selector:
        matchLabels:
          app: ${RESOURCE_NAME}
          version: v1
      template:
        metadata:
          labels:
            app: ${RESOURCE_NAME}
            user_id: ${OWNER}
            team: ${TEAM}
            product: ${PRODUCT}
            created_by: ${CREATED_BY}
            owner: ${OWNER}
            env: circleci
            managed-by: llm-engine
            use_scale_llm_engine_endpoint_network_policy: "true"
            tags.datadoghq.com/env: circleci
            tags.datadoghq.com/version: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
            tags.datadoghq.com/service: ${ENDPOINT_NAME}
            endpoint_id: ${ENDPOINT_ID}
            endpoint_name: ${ENDPOINT_NAME}
            sidecar.istio.io/inject: "false"  # TODO: switch to scuttle
            version: v1
          annotations:
            ad.datadoghq.com/main.logs: '[{"service": "${ENDPOINT_NAME}", "source": "python"}]'
            kubernetes.io/change-cause: "${CHANGE_CAUSE_MESSAGE}"
        spec:
          affinity:
            podAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 1
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: app
                      operator: In
                      values:
                      - ${RESOURCE_NAME}
                  topologyKey: kubernetes.io/hostname
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: ${IMAGE_HASH}
                      operator: In
                      values:
                      - "True"
                  topologyKey: kubernetes.io/hostname
          terminationGracePeriodSeconds: 600
          serviceAccount: default
          nodeSelector:
            node-lifecycle: normal
            k8s.amazonaws.com/accelerator: ${GPU_TYPE}
          tolerations:
            - key: "nvidia.com/gpu"
              operator: "Exists"
              effect: "NoSchedule"
          priorityClassName: ${PRIORITY}
          containers:
            - image: ${IMAGE}
              imagePullPolicy: IfNotPresent
              name: main
              securityContext:
                capabilities:
                  drop:
                  - all
              env:
                - name: DATADOG_TRACE_ENABLED
                  value: "${DATADOG_TRACE_ENABLED}"
                - name: DD_SERVICE
                  value: "${ENDPOINT_NAME}"
                - name: DD_ENV
                  value: circleci
                - name: DD_VERSION
                  value: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
                - name: DD_AGENT_HOST
                  valueFrom:
                    fieldRef:
                      fieldPath: status.hostIP
                - name: OMP_NUM_THREADS
                  value: "1"
                - name: BASE_PATH
                  value: "${BASE_PATH}"
                - name: BUNDLE_URL
                  value: "${BUNDLE_URL}"
                - name: LOAD_PREDICT_FN_MODULE_PATH
                  value: "${LOAD_PREDICT_FN_MODULE_PATH}"
                - name: LOAD_MODEL_FN_MODULE_PATH
                  value: "${LOAD_MODEL_FN_MODULE_PATH}"
                - name: AWS_PROFILE
                  value: "${AWS_ROLE}"
                - name: RESULTS_S3_BUCKET
                  value: "${RESULTS_S3_BUCKET}"
                - name: CHILD_FN_INFO
                  value: "${CHILD_FN_INFO}"
                - name: PREWARM
                  value: "${PREWARM}"
                - name: ML_INFRA_SERVICES_CONFIG_PATH
                  value: "${BASE_PATH}/ml_infra_core/llm_engine.core/llm_engine.core/configs/config.yaml"
                - name: CELERY_S3_BUCKET
                  value: "${CELERY_S3_BUCKET}"
                - name: BROKER_TYPE
                  value: "${BROKER_TYPE}"
                - name: SQS_PROFILE
                  value: "${SQS_PROFILE}"
                - name: SQS_QUEUE_NAME
                  value: "${QUEUE}"
                - name: SQS_QUEUE_URL
                  value: "${SQS_QUEUE_URL}"
              readinessProbe:
                exec:
                  command:
                    - cat
                    - /tmp/readyz
                initialDelaySeconds: 2
                periodSeconds: 2
                failureThreshold: 100
              command: [ "dumb-init", "--", "ddtrace-run" ]
              # Not including --pool=solo means there's a worker process and a separate supervisor process
              # meaning if the worker crashes (because of OOM or something) the supervisor process can mark the task as
              # failed, which should get rid of infinite task retries
              args:
                - celery
                - --app=llm_engine.inference.async_inference
                - worker
                - --loglevel=INFO
                - --concurrency=1
                - --queues=${QUEUE}
                - -O
                - fair
              resources:
                requests:
                  cpu: ${CPUS}
                  memory: ${MEMORY}
                  ${STORAGE_DICT}
                limits:
                  nvidia.com/gpu: ${GPUS}
                  cpu: ${CPUS}
                  memory: ${MEMORY}
                  ${STORAGE_DICT}
              volumeMounts:
                - name: config-volume
                  mountPath: /root/.aws/config
                  subPath: config
                - name: config-volume
                  mountPath: /home/llmengine/.aws/config
                  subPath: config
                - name: user-config
                  mountPath: ${BASE_PATH}/user_config
                  subPath: raw_data
                - name: endpoint-config
                  mountPath: ${BASE_PATH}/endpoint_config
                  subPath: raw_data
                - name: infra-service-config-volume
                  mountPath: ${BASE_PATH}/ml_infra_core/llm_engine.core/llm_engine.core/configs
          # Workaround for https://github.com/kubernetes-sigs/external-dns/pull/1185
          securityContext:
            fsGroup: 65534
          volumes:
            - name: config-volume
              configMap:
                name: default-config  
            - name: user-config
              configMap:
                name: ${RESOURCE_NAME}
            - name: endpoint-config
              configMap:
                name: ${RESOURCE_NAME}-endpoint-config
            - name: dshm
              emptyDir:
                medium: Memory
            - name: infra-service-config-volume
              configMap:
                name: llm-engine-service-config
                items:
                  - key: infra_service_config
                    path: config.yaml
  deployment-triton-enhanced-runnable-image-sync-gpu.yaml: |-
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: ${RESOURCE_NAME}
      namespace: ${NAMESPACE}
      labels:
        user_id: ${OWNER}
        team: ${TEAM}
        product: ${PRODUCT}
        created_by: ${CREATED_BY}
        owner: ${OWNER}
        env: circleci
        managed-by: llm-engine
        use_scale_llm_engine_endpoint_network_policy: "true"
        tags.datadoghq.com/env: circleci
        tags.datadoghq.com/version: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
        tags.datadoghq.com/service: ${ENDPOINT_NAME}
        endpoint_id: ${ENDPOINT_ID}
        endpoint_name: ${ENDPOINT_NAME}
    spec:
      strategy:
        type: RollingUpdate
        rollingUpdate:
          maxSurge: 1
          maxUnavailable: 0
      replicas: ${MIN_WORKERS}
      selector:
        matchLabels:
          app: ${RESOURCE_NAME}
          version: v1
      template:
        metadata:
          labels:
            app: ${RESOURCE_NAME}
            user_id: ${OWNER}
            team: ${TEAM}
            product: ${PRODUCT}
            created_by: ${CREATED_BY}
            owner: ${OWNER}
            env: circleci
            managed-by: llm-engine
            use_scale_llm_engine_endpoint_network_policy: "true"
            tags.datadoghq.com/env: circleci
            tags.datadoghq.com/version: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
            tags.datadoghq.com/service: ${ENDPOINT_NAME}
            endpoint_id: ${ENDPOINT_ID}
            endpoint_name: ${ENDPOINT_NAME}
            version: v1
          annotations:
            ad.datadoghq.com/main.logs: '[{"service": "${ENDPOINT_NAME}", "source": "python"}]'
            kubernetes.io/change-cause: "${CHANGE_CAUSE_MESSAGE}"
        spec:
          affinity:
            podAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 1
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: app
                      operator: In
                      values:
                      - ${RESOURCE_NAME}
                  topologyKey: kubernetes.io/hostname
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: ${IMAGE_HASH}
                      operator: In
                      values:
                      - "True"
                  topologyKey: kubernetes.io/hostname
          terminationGracePeriodSeconds: 600
          serviceAccount: default
          nodeSelector:
            node-lifecycle: normal
            k8s.amazonaws.com/accelerator: ${GPU_TYPE}
          tolerations:
            - key: "nvidia.com/gpu"
              operator: "Exists"
              effect: "NoSchedule"
          priorityClassName: ${PRIORITY}
          containers:
            - name: http-forwarder
              image: 000000000000.dkr.ecr.us-west-2.amazonaws.com/llm-engine:${FORWARDER_IMAGE_TAG}
              imagePullPolicy: IfNotPresent
              command:
                - /usr/bin/dumb-init
                - --
                - ddtrace-run
                - run-service
                - --config
                - /workspace/server/llm_engine_server/inference/configs/${FORWARDER_CONFIG_FILE_NAME}
                - --http
                - production_threads
                - --port
                - "${FORWARDER_PORT}"
                - --concurrency
                - "${PER_WORKER}"
                - --set
                - "forwarder.model.args.predict_route=${PREDICT_ROUTE}"
                - --set
                - "forwarder.model.args.healthcheck_route=${HEALTHCHECK_ROUTE}"
              env:
                - name: DATADOG_TRACE_ENABLED
                  value: "${DATADOG_TRACE_ENABLED}"
                - name: DD_SERVICE
                  value: "${ENDPOINT_NAME}"
                - name: DD_ENV
                  value: circleci
                - name: DD_VERSION
                  value: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
                - name: DD_AGENT_HOST
                  valueFrom:
                    fieldRef:
                      fieldPath: status.hostIP
                - name: AWS_PROFILE
                  value: "${AWS_ROLE}"
                - name: RESULTS_S3_BUCKET
                  value: "${RESULTS_S3_BUCKET}"
                - name: BASE_PATH
                  value: "/workspace"
                - name: ML_INFRA_SERVICES_CONFIG_PATH
                  value: "/workspace/ml_infra_core/llm_engine.core/llm_engine.core/configs/config.yaml"
                - name: HTTP_HOST
                  value: "0.0.0.0"
              readinessProbe:
                httpGet:
                  path: /readyz
                  port: ${FORWARDER_PORT}
                initialDelaySeconds: ${READINESS_INITIAL_DELAY}
                periodSeconds: 5
              resources:
                requests:
                  cpu: 0.1
                  memory: "100M"
                  ephemeral-storage: "100M"
                limits:
                  cpu: ${FORWARDER_CPUS_LIMIT}
                  memory: ${FORWARDER_MEMORY_LIMIT}
                  ephemeral-storage: ${FORWARDER_STORAGE_LIMIT}
              
              
              volumeMounts:
                - name: config-volume
                  mountPath: /root/.aws/config
                  subPath: config
                - name: user-config
                  mountPath: /workspace/user_config
                  subPath: raw_data
                - name: endpoint-config
                  mountPath: /workspace/endpoint_config
                  subPath: raw_data
                - name: infra-service-config-volume
                  mountPath: /workspace/ml_infra_core/llm_engine.core/llm_engine.core/configs
              ports:
                - containerPort: ${FORWARDER_PORT}
                  name: http
            - name: tritonserver
              image: 000000000000.dkr.ecr.us-west-2.amazonaws.com/std-ml-srv:${TRITON_COMMIT_TAG}-triton
              imagePullPolicy: IfNotPresent
              command:
                - /usr/bin/dumb-init
                - --
                - bash
                - -c
                - "$TRITON_COMMAND"
              env:
                - name: AWS_PROFILE
                  value: "${AWS_ROLE}"
              ports:
                - containerPort: 8000
                  name: http
                - containerPort: 8001
                  name: grpc
                - containerPort: 8002
                  name: metrics
              readinessProbe:
                httpGet:
                # Need to have Triton support --http-address IPv6 :(
                # https://github:com/triton-inference-server/server/issues/5305:
                #   path: /v2/health/ready
                #   port: 8000
                  path: /readyz
                  port: 3000
                initialDelaySeconds: $TRITON_READINESS_INITIAL_DELAY
                periodSeconds: 10
              resources:
                requests:
                  cpu: ${TRITON_CPUS}
                  ${TRITON_MEMORY_DICT}
                  ${TRITON_STORAGE_DICT}
                limits:
                  cpu: ${TRITON_CPUS}
                  ${TRITON_MEMORY_DICT}
                  ${TRITON_STORAGE_DICT}
              volumeMounts:
                - name: config-volume
                  mountPath: /root/.aws/config
                  subPath: config
                - mountPath: /dev/shm
                  name: dshm
            - name: main
              securityContext:
                capabilities:
                  drop:
                  - all
              image: ${IMAGE}
              imagePullPolicy: IfNotPresent
              command: ${COMMAND}
              env: ${MAIN_ENV}
              readinessProbe:
                httpGet:
                  path: ${HEALTHCHECK_ROUTE}
                  port: ${USER_CONTAINER_PORT}
                initialDelaySeconds: ${READINESS_INITIAL_DELAY}
                periodSeconds: 5
              resources:
                requests:
                  cpu: ${CPUS}
                  memory: ${MEMORY}
                  ${STORAGE_DICT}
                limits:
                  nvidia.com/gpu: ${GPUS}
                  cpu: ${CPUS}
                  memory: ${MEMORY}
                  ${STORAGE_DICT}
              volumeMounts:
                - name: config-volume
                  mountPath: /root/.aws/config
                  subPath: config
                - mountPath: /dev/shm
                  name: dshm
                - name: infra-service-config-volume
                  mountPath: ${INFRA_SERVICE_CONFIG_VOLUME_MOUNT_PATH}
                # LIRA: For compatibility with runnable image converted from artifactlike bundle
                - name: config-volume
                  mountPath: /home/llmengine/.aws/config
                  subPath: config
                - name: user-config
                  mountPath: /app/user_config
                  subPath: raw_data
                - name: endpoint-config
                  mountPath: /app/endpoint_config
                  subPath: raw_data
              ports:
                - containerPort: ${USER_CONTAINER_PORT}
                  name: http
          # Workaround for https://github.com/kubernetes-sigs/external-dns/pull/1185
          securityContext:
            fsGroup: 65534
          volumes:
            - name: config-volume
              configMap:
                name: default-config  
            - name: user-config
              configMap:
                name: ${RESOURCE_NAME}
            - name: endpoint-config
              configMap:
                name: ${RESOURCE_NAME}-endpoint-config
            - name: dshm
              emptyDir:
                medium: Memory
            - name: infra-service-config-volume
              configMap:
                name: llm-engine-service-config
                items:
                  - key: infra_service_config
                    path: config.yaml
  deployment-runnable-image-sync-gpu.yaml: |-
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: ${RESOURCE_NAME}
      namespace: ${NAMESPACE}
      labels:
        user_id: ${OWNER}
        team: ${TEAM}
        product: ${PRODUCT}
        created_by: ${CREATED_BY}
        owner: ${OWNER}
        env: circleci
        managed-by: llm-engine
        use_scale_llm_engine_endpoint_network_policy: "true"
        tags.datadoghq.com/env: circleci
        tags.datadoghq.com/version: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
        tags.datadoghq.com/service: ${ENDPOINT_NAME}
        endpoint_id: ${ENDPOINT_ID}
        endpoint_name: ${ENDPOINT_NAME}
    spec:
      strategy:
        type: RollingUpdate
        rollingUpdate:
          maxSurge: 1
          maxUnavailable: 0
      replicas: ${MIN_WORKERS}
      selector:
        matchLabels:
          app: ${RESOURCE_NAME}
          version: v1
      template:
        metadata:
          labels:
            app: ${RESOURCE_NAME}
            user_id: ${OWNER}
            team: ${TEAM}
            product: ${PRODUCT}
            created_by: ${CREATED_BY}
            owner: ${OWNER}
            env: circleci
            managed-by: llm-engine
            use_scale_llm_engine_endpoint_network_policy: "true"
            tags.datadoghq.com/env: circleci
            tags.datadoghq.com/version: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
            tags.datadoghq.com/service: ${ENDPOINT_NAME}
            endpoint_id: ${ENDPOINT_ID}
            endpoint_name: ${ENDPOINT_NAME}
            version: v1
          annotations:
            ad.datadoghq.com/main.logs: '[{"service": "${ENDPOINT_NAME}", "source": "python"}]'
            kubernetes.io/change-cause: "${CHANGE_CAUSE_MESSAGE}"
        spec:
          affinity:
            podAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 1
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: app
                      operator: In
                      values:
                      - ${RESOURCE_NAME}
                  topologyKey: kubernetes.io/hostname
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: ${IMAGE_HASH}
                      operator: In
                      values:
                      - "True"
                  topologyKey: kubernetes.io/hostname
          terminationGracePeriodSeconds: 600
          serviceAccount: default
          nodeSelector:
            node-lifecycle: normal
            k8s.amazonaws.com/accelerator: ${GPU_TYPE}
          tolerations:
            - key: "nvidia.com/gpu"
              operator: "Exists"
              effect: "NoSchedule"
          priorityClassName: ${PRIORITY}
          containers:
            - name: http-forwarder
              image: 000000000000.dkr.ecr.us-west-2.amazonaws.com/llm-engine:${FORWARDER_IMAGE_TAG}
              imagePullPolicy: IfNotPresent
              command:
                - /usr/bin/dumb-init
                - --
                - ddtrace-run
                - run-service
                - --config
                - /workspace/server/llm_engine_server/inference/configs/${FORWARDER_CONFIG_FILE_NAME}
                - --http
                - production_threads
                - --port
                - "${FORWARDER_PORT}"
                - --concurrency
                - "${PER_WORKER}"
                - --set
                - "forwarder.model.args.predict_route=${PREDICT_ROUTE}"
                - --set
                - "forwarder.model.args.healthcheck_route=${HEALTHCHECK_ROUTE}"
              env:
                - name: DATADOG_TRACE_ENABLED
                  value: "${DATADOG_TRACE_ENABLED}"
                - name: DD_SERVICE
                  value: "${ENDPOINT_NAME}"
                - name: DD_ENV
                  value: circleci
                - name: DD_VERSION
                  value: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
                - name: DD_AGENT_HOST
                  valueFrom:
                    fieldRef:
                      fieldPath: status.hostIP
                - name: AWS_PROFILE
                  value: "${AWS_ROLE}"
                - name: RESULTS_S3_BUCKET
                  value: "${RESULTS_S3_BUCKET}"
                - name: BASE_PATH
                  value: "/workspace"
                - name: ML_INFRA_SERVICES_CONFIG_PATH
                  value: "/workspace/ml_infra_core/llm_engine.core/llm_engine.core/configs/config.yaml"
                - name: HTTP_HOST
                  value: "0.0.0.0"
              readinessProbe:
                httpGet:
                  path: /readyz
                  port: ${FORWARDER_PORT}
                initialDelaySeconds: ${READINESS_INITIAL_DELAY}
                periodSeconds: 5
              resources:
                requests:
                  cpu: 0.1
                  memory: "100M"
                  ephemeral-storage: "100M"
                limits:
                  cpu: ${FORWARDER_CPUS_LIMIT}
                  memory: ${FORWARDER_MEMORY_LIMIT}
                  ephemeral-storage: ${FORWARDER_STORAGE_LIMIT}
              
              
              volumeMounts:
                - name: config-volume
                  mountPath: /root/.aws/config
                  subPath: config
                - name: user-config
                  mountPath: /workspace/user_config
                  subPath: raw_data
                - name: endpoint-config
                  mountPath: /workspace/endpoint_config
                  subPath: raw_data
                - name: infra-service-config-volume
                  mountPath: /workspace/ml_infra_core/llm_engine.core/llm_engine.core/configs
              ports:
                - containerPort: ${FORWARDER_PORT}
                  name: http
            - name: main
              securityContext:
                capabilities:
                  drop:
                  - all
              image: ${IMAGE}
              imagePullPolicy: IfNotPresent
              command: ${COMMAND}
              env: ${MAIN_ENV}
              readinessProbe:
                httpGet:
                  path: ${HEALTHCHECK_ROUTE}
                  port: ${USER_CONTAINER_PORT}
                initialDelaySeconds: ${READINESS_INITIAL_DELAY}
                periodSeconds: 5
              resources:
                requests:
                  cpu: ${CPUS}
                  memory: ${MEMORY}
                  ${STORAGE_DICT}
                limits:
                  nvidia.com/gpu: ${GPUS}
                  cpu: ${CPUS}
                  memory: ${MEMORY}
                  ${STORAGE_DICT}
              volumeMounts:
                - name: config-volume
                  mountPath: /root/.aws/config
                  subPath: config
                - mountPath: /dev/shm
                  name: dshm
                - name: infra-service-config-volume
                  mountPath: ${INFRA_SERVICE_CONFIG_VOLUME_MOUNT_PATH}
                # LIRA: For compatibility with runnable image converted from artifactlike bundle
                - name: config-volume
                  mountPath: /home/llmengine/.aws/config
                  subPath: config
                - name: user-config
                  mountPath: /app/user_config
                  subPath: raw_data
                - name: endpoint-config
                  mountPath: /app/endpoint_config
                  subPath: raw_data
              ports:
                - containerPort: ${USER_CONTAINER_PORT}
                  name: http
          # Workaround for https://github.com/kubernetes-sigs/external-dns/pull/1185
          securityContext:
            fsGroup: 65534
          volumes:
            - name: config-volume
              configMap:
                name: default-config  
            - name: user-config
              configMap:
                name: ${RESOURCE_NAME}
            - name: endpoint-config
              configMap:
                name: ${RESOURCE_NAME}-endpoint-config
            - name: dshm
              emptyDir:
                medium: Memory
            - name: infra-service-config-volume
              configMap:
                name: llm-engine-service-config
                items:
                  - key: infra_service_config
                    path: config.yaml
  deployment-artifact-sync-gpu.yaml: |-
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: ${RESOURCE_NAME}
      namespace: ${NAMESPACE}
      labels:
        user_id: ${OWNER}
        team: ${TEAM}
        product: ${PRODUCT}
        created_by: ${CREATED_BY}
        owner: ${OWNER}
        env: circleci
        managed-by: llm-engine
        use_scale_llm_engine_endpoint_network_policy: "true"
        tags.datadoghq.com/env: circleci
        tags.datadoghq.com/version: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
        tags.datadoghq.com/service: ${ENDPOINT_NAME}
        endpoint_id: ${ENDPOINT_ID}
        endpoint_name: ${ENDPOINT_NAME}
    spec:
      strategy:
        type: RollingUpdate
        rollingUpdate:
          maxSurge: 1
          maxUnavailable: 0
      replicas: ${MIN_WORKERS}
      selector:
        matchLabels:
          app: ${RESOURCE_NAME}
          version: v1
      template:
        metadata:
          labels:
            app: ${RESOURCE_NAME}
            user_id: ${OWNER}
            team: ${TEAM}
            product: ${PRODUCT}
            created_by: ${CREATED_BY}
            owner: ${OWNER}
            env: circleci
            managed-by: llm-engine
            use_scale_llm_engine_endpoint_network_policy: "true"
            tags.datadoghq.com/env: circleci
            tags.datadoghq.com/version: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
            tags.datadoghq.com/service: ${ENDPOINT_NAME}
            endpoint_id: ${ENDPOINT_ID}
            endpoint_name: ${ENDPOINT_NAME}
            version: v1
          annotations:
            ad.datadoghq.com/main.logs: '[{"service": "${ENDPOINT_NAME}", "source": "python"}]'
            kubernetes.io/change-cause: "${CHANGE_CAUSE_MESSAGE}"
        spec:
          affinity:
            podAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 1
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: app
                      operator: In
                      values:
                      - ${RESOURCE_NAME}
                  topologyKey: kubernetes.io/hostname
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: ${IMAGE_HASH}
                      operator: In
                      values:
                      - "True"
                  topologyKey: kubernetes.io/hostname
          terminationGracePeriodSeconds: 600
          serviceAccount: default
          nodeSelector:
            node-lifecycle: normal
            k8s.amazonaws.com/accelerator: ${GPU_TYPE}
          tolerations:
            - key: "nvidia.com/gpu"
              operator: "Exists"
              effect: "NoSchedule"
          priorityClassName: ${PRIORITY}
          containers:
            - image: ${IMAGE}
              imagePullPolicy: IfNotPresent
              name: main
              securityContext:
                capabilities:
                  drop:
                  - all
              env:
                - name: DATADOG_TRACE_ENABLED
                  value: "${DATADOG_TRACE_ENABLED}"
                - name: DD_SERVICE
                  value: "${ENDPOINT_NAME}"
                - name: DD_ENV
                  value: circleci
                - name: DD_VERSION
                  value: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
                - name: DD_AGENT_HOST
                  valueFrom:
                    fieldRef:
                      fieldPath: status.hostIP
                - name: OMP_NUM_THREADS
                  value: "1"
                - name: BASE_PATH
                  value: "${BASE_PATH}"
                - name: BUNDLE_URL
                  value: "${BUNDLE_URL}"
                - name: LOAD_PREDICT_FN_MODULE_PATH
                  value: "${LOAD_PREDICT_FN_MODULE_PATH}"
                - name: LOAD_MODEL_FN_MODULE_PATH
                  value: "${LOAD_MODEL_FN_MODULE_PATH}"
                - name: AWS_PROFILE
                  value: "${AWS_ROLE}"
                - name: RESULTS_S3_BUCKET
                  value: "${RESULTS_S3_BUCKET}"
                - name: CHILD_FN_INFO
                  value: "${CHILD_FN_INFO}"
                - name: PREWARM
                  value: "${PREWARM}"
                - name: ML_INFRA_SERVICES_CONFIG_PATH
                  value: "${BASE_PATH}/ml_infra_core/llm_engine.core/llm_engine.core/configs/config.yaml"
                - name: PORT
                  value: "${ARTIFACT_LIKE_CONTAINER_PORT}"
              readinessProbe:
                httpGet:
                  path: /readyz
                  port: ${ARTIFACT_LIKE_CONTAINER_PORT}
                initialDelaySeconds: 2
                periodSeconds: 2
                failureThreshold: 100
              command: [ "dumb-init", "--", "ddtrace-run" ]
              args:
                - python
                - -m
                - llm_engine.inference.sync_inference.start_fastapi_server
              resources:
                requests:
                  cpu: ${CPUS}
                  memory: ${MEMORY}
                  ${STORAGE_DICT}
                limits:
                  nvidia.com/gpu: ${GPUS}
                  cpu: ${CPUS}
                  memory: ${MEMORY}
                  ${STORAGE_DICT}
              volumeMounts:
                - name: config-volume
                  mountPath: /root/.aws/config
                  subPath: config
                - name: config-volume
                  mountPath: /home/llmengine/.aws/config
                  subPath: config
                - name: user-config
                  mountPath: ${BASE_PATH}/user_config
                  subPath: raw_data
                - name: endpoint-config
                  mountPath: ${BASE_PATH}/endpoint_config
                  subPath: raw_data
                - name: infra-service-config-volume
                  mountPath: ${BASE_PATH}/ml_infra_core/llm_engine.core/llm_engine.core/configs
          # Workaround for https://github.com/kubernetes-sigs/external-dns/pull/1185
          securityContext:
            fsGroup: 65534
          volumes:
            - name: config-volume
              configMap:
                name: default-config  
            - name: user-config
              configMap:
                name: ${RESOURCE_NAME}
            - name: endpoint-config
              configMap:
                name: ${RESOURCE_NAME}-endpoint-config
            - name: dshm
              emptyDir:
                medium: Memory
            - name: infra-service-config-volume
              configMap:
                name: llm-engine-service-config
                items:
                  - key: infra_service_config
                    path: config.yaml
  deployment-runnable-image-streaming-gpu.yaml: |-
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: ${RESOURCE_NAME}
      namespace: ${NAMESPACE}
      labels:
        user_id: ${OWNER}
        team: ${TEAM}
        product: ${PRODUCT}
        created_by: ${CREATED_BY}
        owner: ${OWNER}
        env: circleci
        managed-by: llm-engine
        use_scale_llm_engine_endpoint_network_policy: "true"
        tags.datadoghq.com/env: circleci
        tags.datadoghq.com/version: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
        tags.datadoghq.com/service: ${ENDPOINT_NAME}
        endpoint_id: ${ENDPOINT_ID}
        endpoint_name: ${ENDPOINT_NAME}
    spec:
      strategy:
        type: RollingUpdate
        rollingUpdate:
          maxSurge: 1
          maxUnavailable: 0
      replicas: ${MIN_WORKERS}
      selector:
        matchLabels:
          app: ${RESOURCE_NAME}
          version: v1
      template:
        metadata:
          labels:
            app: ${RESOURCE_NAME}
            user_id: ${OWNER}
            team: ${TEAM}
            product: ${PRODUCT}
            created_by: ${CREATED_BY}
            owner: ${OWNER}
            env: circleci
            managed-by: llm-engine
            use_scale_llm_engine_endpoint_network_policy: "true"
            tags.datadoghq.com/env: circleci
            tags.datadoghq.com/version: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
            tags.datadoghq.com/service: ${ENDPOINT_NAME}
            endpoint_id: ${ENDPOINT_ID}
            endpoint_name: ${ENDPOINT_NAME}
            version: v1
          annotations:
            ad.datadoghq.com/main.logs: '[{"service": "${ENDPOINT_NAME}", "source": "python"}]'
            kubernetes.io/change-cause: "${CHANGE_CAUSE_MESSAGE}"
        spec:
          affinity:
            podAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 1
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: app
                      operator: In
                      values:
                      - ${RESOURCE_NAME}
                  topologyKey: kubernetes.io/hostname
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: ${IMAGE_HASH}
                      operator: In
                      values:
                      - "True"
                  topologyKey: kubernetes.io/hostname
          terminationGracePeriodSeconds: 600
          serviceAccount: default
          nodeSelector:
            node-lifecycle: normal
            k8s.amazonaws.com/accelerator: ${GPU_TYPE}
          tolerations:
            - key: "nvidia.com/gpu"
              operator: "Exists"
              effect: "NoSchedule"
          priorityClassName: ${PRIORITY}
          containers:
            - name: http-forwarder
              image: 000000000000.dkr.ecr.us-west-2.amazonaws.com/llm-engine:${FORWARDER_IMAGE_TAG}
              imagePullPolicy: IfNotPresent
              command:
                - /usr/bin/dumb-init
                - --
                - ddtrace-run
                - python
                - -m
                - server.llm_engine_server.inference.forwarding.http_forwarder
                - --config
                - /workspace/server/llm_engine_server/inference/configs/service--http_forwarder.yaml
                - --port
                - "${FORWARDER_PORT}"
                - --num-workers
                - "${PER_WORKER}"
                - --set
                - "forwarder.sync.predict_route=${PREDICT_ROUTE}"
                - --set
                - "forwarder.stream.predict_route=${STREAMING_PREDICT_ROUTE}"
                - --set
                - "forwarder.sync.healthcheck_route=${HEALTHCHECK_ROUTE}"
                - --set
                - "forwarder.stream.healthcheck_route=${HEALTHCHECK_ROUTE}"
              env:
                - name: DATADOG_TRACE_ENABLED
                  value: "${DATADOG_TRACE_ENABLED}"
                - name: DD_SERVICE
                  value: "${ENDPOINT_NAME}"
                - name: DD_ENV
                  value: circleci
                - name: DD_VERSION
                  value: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
                - name: DD_AGENT_HOST
                  valueFrom:
                    fieldRef:
                      fieldPath: status.hostIP
                - name: AWS_PROFILE
                  value: "${AWS_ROLE}"
                - name: RESULTS_S3_BUCKET
                  value: "${RESULTS_S3_BUCKET}"
                - name: BASE_PATH
                  value: "/workspace"
                - name: ML_INFRA_SERVICES_CONFIG_PATH
                  value: "/workspace/ml_infra_core/llm_engine.core/llm_engine.core/configs/config.yaml"
                - name: HTTP_HOST
                  value: "0.0.0.0"
              readinessProbe:
                httpGet:
                  path: /readyz
                  port: ${FORWARDER_PORT}
                initialDelaySeconds: ${READINESS_INITIAL_DELAY}
                periodSeconds: 5
              resources:
                requests:
                  cpu: 0.1
                  memory: "100M"
                  ephemeral-storage: "100M"
                limits:
                  cpu: ${FORWARDER_CPUS_LIMIT}
                  memory: ${FORWARDER_MEMORY_LIMIT}
                  ephemeral-storage: ${FORWARDER_STORAGE_LIMIT}
              
              
              volumeMounts:
                - name: config-volume
                  mountPath: /root/.aws/config
                  subPath: config
                - name: user-config
                  mountPath: /workspace/user_config
                  subPath: raw_data
                - name: endpoint-config
                  mountPath: /workspace/endpoint_config
                  subPath: raw_data
                - name: infra-service-config-volume
                  mountPath: /workspace/ml_infra_core/llm_engine.core/llm_engine.core/configs
              ports:
                - containerPort: ${FORWARDER_PORT}
                  name: http
            - name: main
              securityContext:
                capabilities:
                  drop:
                  - all
              image: ${IMAGE}
              imagePullPolicy: IfNotPresent
              command: ${COMMAND}
              env: ${MAIN_ENV}
              readinessProbe:
                httpGet:
                  path: ${HEALTHCHECK_ROUTE}
                  port: ${USER_CONTAINER_PORT}
                initialDelaySeconds: ${READINESS_INITIAL_DELAY}
                periodSeconds: 5
              resources:
                requests:
                  cpu: ${CPUS}
                  memory: ${MEMORY}
                  ${STORAGE_DICT}
                limits:
                  nvidia.com/gpu: ${GPUS}
                  cpu: ${CPUS}
                  memory: ${MEMORY}
                  ${STORAGE_DICT}
              volumeMounts:
                - name: config-volume
                  mountPath: /root/.aws/config
                  subPath: config
                - mountPath: /dev/shm
                  name: dshm
                - name: infra-service-config-volume
                  mountPath: ${INFRA_SERVICE_CONFIG_VOLUME_MOUNT_PATH}
                # LIRA: For compatibility with runnable image converted from artifactlike bundle
                - name: config-volume
                  mountPath: /home/llmengine/.aws/config
                  subPath: config
                - name: user-config
                  mountPath: /app/user_config
                  subPath: raw_data
                - name: endpoint-config
                  mountPath: /app/endpoint_config
                  subPath: raw_data
              ports:
                - containerPort: ${USER_CONTAINER_PORT}
                  name: http
          # Workaround for https://github.com/kubernetes-sigs/external-dns/pull/1185
          securityContext:
            fsGroup: 65534
          volumes:
            - name: config-volume
              configMap:
                name: default-config  
            - name: user-config
              configMap:
                name: ${RESOURCE_NAME}
            - name: endpoint-config
              configMap:
                name: ${RESOURCE_NAME}-endpoint-config
            - name: dshm
              emptyDir:
                medium: Memory
            - name: infra-service-config-volume
              configMap:
                name: llm-engine-service-config
                items:
                  - key: infra_service_config
                    path: config.yaml
  user-config.yaml: |-
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: ${RESOURCE_NAME}
      namespace: ${NAMESPACE}
      labels:
        user_id: ${OWNER}
        team: ${TEAM}
        product: ${PRODUCT}
        created_by: ${CREATED_BY}
        owner: ${OWNER}
        env: circleci
        managed-by: llm-engine
        use_scale_llm_engine_endpoint_network_policy: "true"
        tags.datadoghq.com/env: circleci
        tags.datadoghq.com/version: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
        tags.datadoghq.com/service: ${ENDPOINT_NAME}
        endpoint_id: ${ENDPOINT_ID}
        endpoint_name: ${ENDPOINT_NAME}
    data:
      raw_data: ${CONFIG_DATA_SERIALIZED}
  endpoint-config.yaml: |-
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: ${RESOURCE_NAME}-endpoint-config
      namespace: ${NAMESPACE}
      labels:
        user_id: ${OWNER}
        team: ${TEAM}
        product: ${PRODUCT}
        created_by: ${CREATED_BY}
        owner: ${OWNER}
        env: circleci
        managed-by: llm-engine
        use_scale_llm_engine_endpoint_network_policy: "true"
        tags.datadoghq.com/env: circleci
        tags.datadoghq.com/version: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
        tags.datadoghq.com/service: ${ENDPOINT_NAME}
        endpoint_id: ${ENDPOINT_ID}
        endpoint_name: ${ENDPOINT_NAME}
    data:
      raw_data: ${ENDPOINT_CONFIG_SERIALIZED}
  horizontal-pod-autoscaler.yaml: |-
    apiVersion: ${API_VERSION}
    kind: HorizontalPodAutoscaler
    metadata:
      name: ${RESOURCE_NAME}
      namespace: ${NAMESPACE}
      labels:
        user_id: ${OWNER}
        team: ${TEAM}
        product: ${PRODUCT}
        created_by: ${CREATED_BY}
        owner: ${OWNER}
        env: circleci
        managed-by: llm-engine
        use_scale_llm_engine_endpoint_network_policy: "true"
        tags.datadoghq.com/env: circleci
        tags.datadoghq.com/version: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
        tags.datadoghq.com/service: ${ENDPOINT_NAME}
        endpoint_id: ${ENDPOINT_ID}
        endpoint_name: ${ENDPOINT_NAME}
    spec:
      minReplicas: ${MIN_WORKERS}
      maxReplicas: ${MAX_WORKERS}
      scaleTargetRef:
        apiVersion: apps/v1
        kind: Deployment
        name: ${RESOURCE_NAME}
      metrics:
        - type: Pods
          pods:
            metric:
              name: request-concurrency-average
            target:
              type: Value
              averageValue: ${CONCURRENCY}
  service.yaml: |-
    apiVersion: v1
    kind: Service
    metadata:
      name: ${RESOURCE_NAME}
      namespace: ${NAMESPACE}
      labels:
        user_id: ${OWNER}
        team: ${TEAM}
        product: ${PRODUCT}
        created_by: ${CREATED_BY}
        owner: ${OWNER}
        env: circleci
        managed-by: llm-engine
        use_scale_llm_engine_endpoint_network_policy: "true"
        tags.datadoghq.com/env: circleci
        tags.datadoghq.com/version: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
        tags.datadoghq.com/service: ${ENDPOINT_NAME}
        endpoint_id: ${ENDPOINT_ID}
        endpoint_name: ${ENDPOINT_NAME}
    spec:
      type: ${SERVICE_TYPE}
      selector:
        app: ${RESOURCE_NAME}
      ports:
        - port: 80
          targetPort: ${SERVICE_TARGET_PORT}
          protocol: TCP
          name: http
          ${NODE_PORT_DICT}
  vertical-pod-autoscaler.yaml: |-
    apiVersion: "autoscaling.k8s.io/v1"
    kind: VerticalPodAutoscaler
    metadata:
      name: ${RESOURCE_NAME}
      labels:
        user_id: ${OWNER}
        team: ${TEAM}
        product: ${PRODUCT}
        created_by: ${CREATED_BY}
        owner: ${OWNER}
        env: circleci
        managed-by: llm-engine
        use_scale_llm_engine_endpoint_network_policy: "true"
        tags.datadoghq.com/env: circleci
        tags.datadoghq.com/version: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
        tags.datadoghq.com/service: ${ENDPOINT_NAME}
        endpoint_id: ${ENDPOINT_ID}
        endpoint_name: ${ENDPOINT_NAME}
    spec:
      targetRef:
        apiVersion: "apps/v1"
        kind: Deployment
        name: ${RESOURCE_NAME}
      updatePolicy:
        updateMode: "Auto"
      resourcePolicy:
        containerPolicies:
          - containerName: istio-proxy
            mode: "Off"
          - containerName: main
            minAllowed:
              cpu: 100m
              memory: 128Mi
            maxAllowed:
              cpu: ${CPUS}
              memory: ${MEMORY}
            controlledResources: ["cpu", "memory"]
  batch-job-orchestration-job.yaml: |-
    apiVersion: batch/v1
    kind: Job
    metadata:
      name: ${RESOURCE_NAME}
      labels:
        user_id: ${OWNER}
        team: ${TEAM}
        product: ${PRODUCT}
        created_by: ${CREATED_BY}
        owner: ${OWNER}
        env: circleci
        managed-by: llm-engine
        use_scale_llm_engine_endpoint_network_policy: "true"
        tags.datadoghq.com/env: circleci
        tags.datadoghq.com/version: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
        llm_engine_job_id: ${JOB_ID}
        tags.datadoghq.com/service: ${JOB_ID}
    spec:
      backoffLimit: 0
      activeDeadlineSeconds: ${BATCH_JOB_MAX_RUNTIME}
      ttlSecondsAfterFinished: ${BATCH_JOB_TTL_SECONDS_AFTER_FINISHED}
      template:
        metadata:
          labels:
            user_id: ${OWNER}
            team: ${TEAM}
            product: ${PRODUCT}
            created_by: ${CREATED_BY}
            owner: ${OWNER}
            env: circleci
            managed-by: llm-engine
            use_scale_llm_engine_endpoint_network_policy: "true"
            tags.datadoghq.com/env: circleci
            tags.datadoghq.com/version: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
            llm_engine_job_id: ${JOB_ID}
            tags.datadoghq.com/service: ${JOB_ID}
            sidecar.istio.io/inject: "false"
            version: v1
          annotations:
            ad.datadoghq.com/main.logs: '[{"source": "python", "service": "${RESOURCE_NAME}", "tags": ["env:circleci", "llm_engine_job_id:${JOB_ID}"]}]'
            cluster-autoscaler.kubernetes.io/safe-to-evict: "false"
        spec:
          restartPolicy: Never
          nodeSelector:
            node-lifecycle: normal
          serviceAccountName: llm-engine
          volumes:
            - name: config-volume
              configMap:
                name: default-config
          containers:
            - name: main
              image: 000000000000.dkr.ecr.us-west-2.amazonaws.com/llm-engine:54f8f73bfb1cce62a2b42326ccf9f49b5b145126
              env:
                - name: DD_SERVICE
                  value: ${RESOURCE_NAME}
                - name: DATADOG_TRACE_ENABLED
                  value: "true"
                - name: DD_ENV
                  value: circleci
                - name: DD_VERSION
                  value: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
                - name: DD_AGENT_HOST
                  valueFrom:
                    fieldRef:
                      fieldPath: status.hostIP
                - name: GIT_TAG
                  value: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
                - name: SERVICE_IDENTIFIER
                - name: AWS_PROFILE
                  value: default
                - name: ECR_READ_AWS_PROFILE
                  value: default
                - name: ML_INFRA_DATABASE_URL
                  valueFrom:
                    secretKeyRef:
                      key: database_url
                      name: ml-infra-pg
                - name: DEPLOY_SERVICE_CONFIG_PATH
                  value: /workspace/llm_engine/service_configs/service_config.yaml
                - name: ML_INFRA_SERVICES_CONFIG_PATH
                  value: /workspace/ml_infra_core/llm_engine.core/llm_engine.core/configs/config.yaml
                - name: CELERY_ELASTICACHE_ENABLED
                  value: "true"
                - name: LLM_ENGINE_SERVICE_TEMPLATE_FOLDER
                  value: /workspace/llm_engine/llm_engine/infra/gateways/resources/templates
              imagePullPolicy: Always
              command:
                - dumb-init
                - --
                - ddtrace-run
              args:
                - python
                - -m
                - server.llm_engine_server.entrypoints.start_batch_job_orchestration
                - --job-id
                - ${JOB_ID}
                - --owner
                - ${OWNER}
                - --input-path
                - ${INPUT_LOCATION}
                - --serialization-format
                - ${SERIALIZATION_FORMAT}
                - --timeout-seconds
                - "${BATCH_JOB_TIMEOUT}"
              resources:
                # If job pods get evicted, then we can make "Guaranteed QoS" by setting requests = limits.
                requests:
                  cpu: 1
                  memory: 8Gi
                limits:
                  cpu: 4
                  memory: 32Gi
              volumeMounts:
                - name: config-volume
                  mountPath: /root/.aws/config
                  subPath: config
  docker-image-batch-job-cpu.yaml: |-
    apiVersion: batch/v1
    kind: Job
    metadata:
      name: ${RESOURCE_NAME}
      labels:
        user_id: ${OWNER}
        team: ${TEAM}
        product: ${PRODUCT}
        created_by: ${CREATED_BY}
        owner: ${OWNER}
        env: circleci
        managed-by: llm-engine
        use_scale_llm_engine_endpoint_network_policy: "true"
        tags.datadoghq.com/env: circleci
        tags.datadoghq.com/version: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
        llm_engine_job_id: ${JOB_ID}
        tags.datadoghq.com/service: ${JOB_ID}
    spec:
      backoffLimit: 0
      activeDeadlineSeconds: ${BATCH_JOB_MAX_RUNTIME}
      ttlSecondsAfterFinished: ${BATCH_JOB_TTL_SECONDS_AFTER_FINISHED}
      template:
        metadata:
          labels:
            user_id: ${OWNER}
            team: ${TEAM}
            product: ${PRODUCT}
            created_by: ${CREATED_BY}
            owner: ${OWNER}
            env: circleci
            managed-by: llm-engine
            use_scale_llm_engine_endpoint_network_policy: "true"
            tags.datadoghq.com/env: circleci
            tags.datadoghq.com/version: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
            llm_engine_job_id: ${JOB_ID}
            tags.datadoghq.com/service: ${JOB_ID}
            sidecar.istio.io/inject: "false"
            version: v1
          annotations:
            ad.datadoghq.com/main.logs: '[{"source": "python", "service": "${RESOURCE_NAME}", "tags": ["env:circleci", "llm_engine_job_id:${JOB_ID}"]}]'
        spec:
          restartPolicy: Never
          nodeSelector:
            node-lifecycle: normal
          serviceAccountName: default
          volumes:
            - name: config-volume
              configMap:
                name: default-config
            - name: workdir
              emptyDir: {}
            - name: dshm
              emptyDir:
                medium: Memory
          containers:
            - name: main
              image: ${IMAGE}
              env:
                - name: DD_SERVICE
                  value: ${RESOURCE_NAME}
                - name: DATADOG_TRACE_ENABLED
                  value: "true"
                - name: DD_ENV
                  value: circleci
                - name: DD_VERSION
                  value: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
                - name: DD_AGENT_HOST
                  valueFrom:
                    fieldRef:
                      fieldPath: status.hostIP
                - name: GIT_TAG
                  value: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
                - name: SERVICE_IDENTIFIER
                - name: AWS_PROFILE
                  value: default
                - name: ECR_READ_AWS_PROFILE
                  value: default
                - name: ML_INFRA_DATABASE_URL
                  valueFrom:
                    secretKeyRef:
                      key: database_url
                      name: ml-infra-pg
                - name: DEPLOY_SERVICE_CONFIG_PATH
                  value: /workspace/llm_engine/service_configs/service_config.yaml
                - name: ML_INFRA_SERVICES_CONFIG_PATH
                  value: /workspace/ml_infra_core/llm_engine.core/llm_engine.core/configs/config.yaml
                - name: CELERY_ELASTICACHE_ENABLED
                  value: "true"
                - name: LLM_ENGINE_SERVICE_TEMPLATE_FOLDER
                  value: /workspace/llm_engine/llm_engine/infra/gateways/resources/templates
              imagePullPolicy: Always
              command: ${COMMAND}
              resources:
                # If job pods get evicted, then we can make "Guaranteed QoS" by setting requests = limits.
                requests:
                  cpu: ${CPUS}
                  memory: ${MEMORY}
                  ${STORAGE_DICT}
                limits:
                  cpu: ${CPUS}
                  memory: ${MEMORY}
                  ${STORAGE_DICT}
              volumeMounts:
                - name: config-volume
                  mountPath: /root/.aws/config
                  subPath: config
                - name: workdir
                  mountPath: ${MOUNT_PATH}
                - mountPath: /dev/shm
                  name: dshm
          initContainers:
            - name: input-downloader
              image: 000000000000.dkr.ecr.us-west-2.amazonaws.com/llm-engine:54f8f73bfb1cce62a2b42326ccf9f49b5b145126
              command:
                - python
                - -m
                - server.llm_engine_server.entrypoints.start_docker_image_batch_job_init_container
                - ${INPUT_LOCATION}
                - --remote-file
                - ${S3_FILE}
                - --local-file
                - ${LOCAL_FILE_NAME}
                - --file-contents-b64encoded
                - ${FILE_CONTENTS_B64ENCODED}
              resources:
                requests:
                  cpu: 1
                  memory: 1Gi
                limits:
                  cpu: 1
                  memory: 1Gi
              volumeMounts:
                - name: config-volume
                  mountPath: /root/.aws/config
                  subPath: config
                - name: workdir
                  mountPath: ${MOUNT_PATH}
  docker-image-batch-job-gpu.yaml: |-
    apiVersion: batch/v1
    kind: Job
    metadata:
      name: ${RESOURCE_NAME}
      labels:
        user_id: ${OWNER}
        team: ${TEAM}
        product: ${PRODUCT}
        created_by: ${CREATED_BY}
        owner: ${OWNER}
        env: circleci
        managed-by: llm-engine
        use_scale_llm_engine_endpoint_network_policy: "true"
        tags.datadoghq.com/env: circleci
        tags.datadoghq.com/version: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
        llm_engine_job_id: ${JOB_ID}
        tags.datadoghq.com/service: ${JOB_ID}
    spec:
      backoffLimit: 0
      activeDeadlineSeconds: ${BATCH_JOB_MAX_RUNTIME}
      ttlSecondsAfterFinished: ${BATCH_JOB_TTL_SECONDS_AFTER_FINISHED}
      template:
        metadata:
          labels:
            user_id: ${OWNER}
            team: ${TEAM}
            product: ${PRODUCT}
            created_by: ${CREATED_BY}
            owner: ${OWNER}
            env: circleci
            managed-by: llm-engine
            use_scale_llm_engine_endpoint_network_policy: "true"
            tags.datadoghq.com/env: circleci
            tags.datadoghq.com/version: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
            llm_engine_job_id: ${JOB_ID}
            tags.datadoghq.com/service: ${JOB_ID}
            sidecar.istio.io/inject: "false"
            version: v1
          annotations:
            ad.datadoghq.com/main.logs: '[{"source": "python", "service": "${RESOURCE_NAME}", "tags": ["env:circleci", "llm_engine_job_id:${JOB_ID}"]}]'
        spec:
          restartPolicy: Never
          nodeSelector:
            node-lifecycle: normal
            k8s.amazonaws.com/accelerator: ${GPU_TYPE}
          tolerations:
            - key: "nvidia.com/gpu"
              operator: "Exists"
              effect: "NoSchedule"
          serviceAccountName: default
          volumes:
            - name: config-volume
              configMap:
                name: default-config
            - name: workdir
              emptyDir: {}
            - name: dshm
              emptyDir:
                medium: Memory
          containers:
            - name: main
              image: ${IMAGE}
              env:
                - name: DD_SERVICE
                  value: ${RESOURCE_NAME}
                - name: DATADOG_TRACE_ENABLED
                  value: "true"
                - name: DD_ENV
                  value: circleci
                - name: DD_VERSION
                  value: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
                - name: DD_AGENT_HOST
                  valueFrom:
                    fieldRef:
                      fieldPath: status.hostIP
                - name: GIT_TAG
                  value: 54f8f73bfb1cce62a2b42326ccf9f49b5b145126
                - name: SERVICE_IDENTIFIER
                - name: AWS_PROFILE
                  value: default
                - name: ECR_READ_AWS_PROFILE
                  value: default
                - name: ML_INFRA_DATABASE_URL
                  valueFrom:
                    secretKeyRef:
                      key: database_url
                      name: ml-infra-pg
                - name: DEPLOY_SERVICE_CONFIG_PATH
                  value: /workspace/llm_engine/service_configs/service_config.yaml
                - name: ML_INFRA_SERVICES_CONFIG_PATH
                  value: /workspace/ml_infra_core/llm_engine.core/llm_engine.core/configs/config.yaml
                - name: CELERY_ELASTICACHE_ENABLED
                  value: "true"
                - name: LLM_ENGINE_SERVICE_TEMPLATE_FOLDER
                  value: /workspace/llm_engine/llm_engine/infra/gateways/resources/templates
              imagePullPolicy: Always
              command: ${COMMAND}
              resources:
                # If job pods get evicted, then we can make "Guaranteed QoS" by setting requests = limits.
                requests:
                  cpu: ${CPUS}
                  memory: ${MEMORY}
                  ${STORAGE_DICT}
                limits:
                  nvidia.com/gpu: ${GPUS}
                  cpu: ${CPUS}
                  memory: ${MEMORY}
                  ${STORAGE_DICT}
              volumeMounts:
                - name: config-volume
                  mountPath: /root/.aws/config
                  subPath: config
                - name: workdir
                  mountPath: ${MOUNT_PATH}
                - mountPath: /dev/shm
                  name: dshm
          initContainers:
            - name: input-downloader
              image: 000000000000.dkr.ecr.us-west-2.amazonaws.com/llm-engine:54f8f73bfb1cce62a2b42326ccf9f49b5b145126
              command:
                - python
                - -m
                - server.llm_engine_server.entrypoints.start_docker_image_batch_job_init_container
                - ${INPUT_LOCATION}
                - --remote-file
                - ${S3_FILE}
                - --local-file
                - ${LOCAL_FILE_NAME}
                - --file-contents-b64encoded
                - ${FILE_CONTENTS_B64ENCODED}
              resources:
                requests:
                  cpu: 1
                  memory: 1Gi
                limits:
                  cpu: 1
                  memory: 1Gi
              volumeMounts:
                - name: config-volume
                  mountPath: /root/.aws/config
                  subPath: config
                - name: workdir
                  mountPath: ${MOUNT_PATH}
  image-cache-cpu.yaml: |-
    apiVersion: apps/v1
    kind: DaemonSet
    metadata:
      name: ${RESOURCE_NAME}
      namespace: ${NAMESPACE}
      labels:
        team: infra
        product: llm-engine
        use_scale_llm_engine_endpoint_network_policy: "true"
        tags.datadoghq.com/service: ${RESOURCE_NAME}
    spec:
      selector:
        matchLabels:
          app: ${RESOURCE_NAME}
          version: v1
      updateStrategy:
        type: RollingUpdate
      template:
        metadata:
          labels:
            app: ${RESOURCE_NAME}
            team: infra
            product: llm-engine
            use_scale_llm_engine_endpoint_network_policy: "true"
            tags.datadoghq.com/service: ${RESOURCE_NAME}
            version: v1
            sidecar.istio.io/inject: "false"
        spec:
          nodeSelector:
            cpu-only: "true"
          containers:
            - image: public.ecr.aws/docker/library/busybox:latest
              imagePullPolicy: IfNotPresent
              name: busybox
              command: ["/bin/sh", "-ec", "while : ; do sleep 30 ; done"]
          terminationGracePeriodSeconds: 0
  image-cache-a10.yaml: |-
    apiVersion: apps/v1
    kind: DaemonSet
    metadata:
      name: ${RESOURCE_NAME}
      namespace: ${NAMESPACE}
      labels:
        team: infra
        product: llm-engine
        use_scale_llm_engine_endpoint_network_policy: "true"
        tags.datadoghq.com/service: ${RESOURCE_NAME}
    spec:
      selector:
        matchLabels:
          app: ${RESOURCE_NAME}
          version: v1
      updateStrategy:
        type: RollingUpdate
      template:
        metadata:
          labels:
            app: ${RESOURCE_NAME}
            team: infra
            product: llm-engine
            use_scale_llm_engine_endpoint_network_policy: "true"
            tags.datadoghq.com/service: ${RESOURCE_NAME}
            version: v1
            sidecar.istio.io/inject: "false"
        spec:
          nodeSelector:
            k8s.amazonaws.com/accelerator: nvidia-ampere-a10
          tolerations:
            - effect: NoSchedule
              key: nvidia.com/gpu
              operator: Exists
          containers:
            - image: public.ecr.aws/docker/library/busybox:latest
              imagePullPolicy: IfNotPresent
              name: busybox
              command: ["/bin/sh", "-ec", "while : ; do sleep 30 ; done"]
          terminationGracePeriodSeconds: 0
  image-cache-a100.yaml: |-
    apiVersion: apps/v1
    kind: DaemonSet
    metadata:
      name: ${RESOURCE_NAME}
      namespace: ${NAMESPACE}
      labels:
        team: infra
        product: llm-engine
        use_scale_llm_engine_endpoint_network_policy: "true"
        tags.datadoghq.com/service: ${RESOURCE_NAME}
    spec:
      selector:
        matchLabels:
          app: ${RESOURCE_NAME}
          version: v1
      updateStrategy:
        type: RollingUpdate
      template:
        metadata:
          labels:
            app: ${RESOURCE_NAME}
            team: infra
            product: llm-engine
            use_scale_llm_engine_endpoint_network_policy: "true"
            tags.datadoghq.com/service: ${RESOURCE_NAME}
            version: v1
            sidecar.istio.io/inject: "false"
        spec:
          nodeSelector:
            k8s.amazonaws.com/accelerator: nvidia-ampere-a100
          tolerations:
            - effect: NoSchedule
              key: nvidia.com/gpu
              operator: Exists
          containers:
            - image: public.ecr.aws/docker/library/busybox:latest
              imagePullPolicy: IfNotPresent
              name: busybox
              command: ["/bin/sh", "-ec", "while : ; do sleep 30 ; done"]
          terminationGracePeriodSeconds: 0
  image-cache-t4.yaml: |-
    apiVersion: apps/v1
    kind: DaemonSet
    metadata:
      name: ${RESOURCE_NAME}
      namespace: ${NAMESPACE}
      labels:
        team: infra
        product: llm-engine
        use_scale_llm_engine_endpoint_network_policy: "true"
        tags.datadoghq.com/service: ${RESOURCE_NAME}
    spec:
      selector:
        matchLabels:
          app: ${RESOURCE_NAME}
          version: v1
      updateStrategy:
        type: RollingUpdate
      template:
        metadata:
          labels:
            app: ${RESOURCE_NAME}
            team: infra
            product: llm-engine
            use_scale_llm_engine_endpoint_network_policy: "true"
            tags.datadoghq.com/service: ${RESOURCE_NAME}
            version: v1
            sidecar.istio.io/inject: "false"
        spec:
          nodeSelector:
            k8s.amazonaws.com/accelerator: nvidia-tesla-t4
          tolerations:
            - effect: NoSchedule
              key: nvidia.com/gpu
              operator: Exists
          containers:
            - image: public.ecr.aws/docker/library/busybox:latest
              imagePullPolicy: IfNotPresent
              name: busybox
              command: ["/bin/sh", "-ec", "while : ; do sleep 30 ; done"]
          terminationGracePeriodSeconds: 0
