forwarder:
  class_name: llm_engine.inference.forwarding.forwarding.LoadStreamingForwarder
  args:
    user_port: 5005
    user_hostname: "localhost"
    predict_route: "/stream"
    healthcheck_route: "/readyz"
    batch_route: null
    llm_engine_unwrap: true
    serialize_results_as_string: false
