forwarder:
    model:
        class_name: llm_engine.inference.forwarding.forwarding.LoadForwarder
        args:
          user_port: 5005
          user_hostname: "localhost"
          use_grpc: false
          predict_route: "/predict"
          healthcheck_route: "/readyz"
          batch_route: null
          llm_engine_unwrap: true
          serialize_results_as_string: true

