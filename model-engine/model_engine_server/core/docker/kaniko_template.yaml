apiVersion: batch/v1
kind: Job
metadata:
  name: $NAME
  namespace: $NAMESPACE
  labels:
    app: kaniko
    team: infra
    product: common
  annotations:
    ad.datadoghq.com/tags: $CUSTOM_TAGS
spec:
  ttlSecondsAfterFinished: 259200 # 3 days
  activeDeadlineSeconds: 43200
  backoffLimit: 0
  template:
    metadata:
      labels:
        app: $NAME
        team: infra
        product: common
      annotations:
        ad.datadoghq.com/tags: $CUSTOM_TAGS
        cluster-autoscaler.kubernetes.io/safe-to-evict: "false"
        sidecar.istio.io/inject: "false"
    spec:
      containers:
        - name: kaniko
          image: gcr.io/kaniko-project/executor:v1.9.1
          args:
            - "--dockerfile=$DOCKERFILE"
            - "--context=s3://$S3_BUCKET/$S3_FILE"
            - "--cache=$USE_CACHE"
            - "--cache-copy-layers=$USE_CACHE"
            - "--cache-run-layers=$USE_CACHE"
            - "--cache-repo=$CACHE_REPO"
            - "--cleanup"
            - "--snapshot-mode=redo"
            - "--use-new-run"
            - "--image-fs-extract-retry=5"
            - "--log-format=json"
          # The --use-new-run flag should fix docker builds eating up a lot of memory and consequently oom/failing
          env:
            - name: AWS_REGION
              value: us-west-2
          # TODO we need to parametrize AWS_REGION
          volumeMounts:
            - name: pipconf
              mountPath: /kaniko/pip
          resources:
            requests:
              cpu: 3.5
              memory: 30Gi
              ephemeral-storage: 80G
            limits:
              cpu: 3.5
              memory: 30Gi
              ephemeral-storage: 80G
      volumes:
        - name: pipconf
          secret:
            secretName: codeartifact-pip-conf
      restartPolicy: Never
      serviceAccountName: kaniko
