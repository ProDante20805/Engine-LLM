# This is a YAML-formatted file.

# tag [required] is the LLM Engine docker image tag
tag: 60ac144c55aad971cdd7f152f4f7816ce2fb7d2f
# context is a user-specified deployment tag. Can be used to 
context: production
image:
  # gatewayRepository [required] is the docker repository to pull the LLM Engine gateway image from
  gatewayRepository: public.ecr.aws/b2z8n5q1/model-engine
  # builderRepository [required] is the docker repository to pull the LLM Engine endpoint builder image from
  builderRepository: public.ecr.aws/b2z8n5q1/model-engine
  # cacherRepository [required] is the docker repository to pull the LLM Engine cacher image from
  cacherRepository: public.ecr.aws/b2z8n5q1/model-engine
  # forwarderRepository [required] is the docker repository to pull the LLM Engine forwarder image from
  forwarderRepository: public.ecr.aws/b2z8n5q1/model-engine
  # pullPolicy is the docker image pull policy
  pullPolicy: Always

secrets:
  # kubernetesDatabaseSecretName or awsDatabaseSecretName [required]
  # is the name of the secret that contains the database credentials
  kubernetesDatabaseSecretName: llm-engine-postgres-credentials

db:
  runDbInitScript: false

# serviceAccount [required] specifies the service account for LLM Engine server deployments (e.g gateway, cache, and builder deployments).
serviceAccount:
  annotations:
    # eks.amazonaws.com/role-arn [required] is the ARN of the IAM role that the service account will assume
    eks.amazonaws.com/role-arn: arn:aws:iam::000000000000:role/k8s-main-llm-engine
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-weight": "-2"
  namespaces: []

imageBuilderServiceAccount:
  create: true
  annotations:
    # eks.amazonaws.com/role-arn [required] is the ARN of the IAM role that the image builder service account will assume. Needs to have ecr permissions
    eks.amazonaws.com/role-arn: arn:aws:iam::000000000000:role/k8s-main-llm-engine-image-builder
  # Reads from serviceAccount.namespaces to determine which namespaces to create the image builder service account in

# service specifies the service configuration for the main LLM Engine server. Users should setup their own ingress controller to expose the service.
service:
  type: ClusterIP
  port: 80

# virtualservice specifies the configuration of an Istio VirtualService
virtualservice:
  enabled: true
  annotations: { }
  hostDomains:
    - llm-engine.domain.com
  gateways:
    - default/internal-gateway

hostDomain:
  prefix: http://

# destinationrule specifies the configuration of an Istio DestinationRule
destinationrule:
  enabled: true
  annotations: { }

# replicaCount specifies the amount of replica pods for each deployment
replicaCount:
  # gateway is the main LLM Engine server deployment
  gateway: 2  
  # cacher is the kubernetes state caching deployment
  cacher: 1
  # builder is the endpoint builder deployment
  builder: 1
  # balloonA10 is a low priority pod deployment for A10 GPU nodes
  balloonA10: 0
  # balloonA100 is a low priority pod deployment for A100 GPU nodes
  balloonA100: 0
  # balloonCpu is a low priority pod deployment for CPU nodes
  balloonCpu: 0
  # balloonT4 is a low priority pod deployment for T4 GPU nodes
  balloonT4: 0

# autoscaling is the autoscaling configuration for LLM Engine server deployments (e.g gateway, cache, and builder deployments)
autoscaling:
  horizontal:
    enabled: true
    minReplicas: 2
    maxReplicas: 10
    targetConcurrency: 50
  vertical:
    enabled: false
  prewarming:
    enabled: false

# for async endpoints, Celery autoscaler scales the number of pods based on number of requests
# num_shards is number of instances of the autoscaler
celery_autoscaler:
  enabled: true
  num_shards: 3

podDisruptionBudget:
  enabled: true
  minAvailable: 1

# resources specify the k8s resources for LLM Engine server deployments (e.g gateway, cache, and builder deployments)
resources:
  requests:
    cpu: 2
# nodeSelector specifies the node selector for LLM Engine server deployments (e.g gateway, cache, and builder deployments)
nodeSelector: { }
# tolerations specifies the tolerations for LLM Engine server deployments (e.g gateway, cache, and builder deployments)
tolerations: [ ]
# affinity specifies the affinity for LLM Engine server deployments (e.g gateway, cache, and builder deployments)
affinity: { }

# aws specifies the AWS configurations (by configMap) for LLM Engine server deployments
aws:
  configMap:
    name: default-config
    create: true
  profileName: default

# serviceTemplate specifies additional flags for model endpoints
serviceTemplate:
  securityContext:
    capabilities:
      drop:
        - all
  mountInfraConfig: true
  # createServiceAccount/serviceAccountName/serviceAccountAnnotations specify whether to create a serviceAccount for
  # inference pods. Assumes the inference pods run in a separate namespace to the LLM Engine control plane.
  createServiceAccount: true
  serviceAccountName: model-engine
  serviceAccountAnnotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::000000000000:role/llm-engine
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-weight": "-2"

# config specifes the `data` field of the service config map
config:
  values:
    infra:
      # k8s_cluster_name [required] is the name of the k8s cluster
      k8s_cluster_name: main_cluster
      # dns_host_domain [required] is the domain name of the k8s cluster
      dns_host_domain: llm-engine.domain.com
      # default_region [required] is the default AWS region for various resources (e.g ECR)
      default_region: us-east-1
      # aws_account_id [required] is the AWS account ID for various resources (e.g ECR)
      ml_account_id: "000000000000"
      # docker_repo_prefix [required] is the prefix for AWS ECR repositories
      docker_repo_prefix: "000000000000.dkr.ecr.us-east-1.amazonaws.com"
      # redis_host [required] is the hostname of the redis cluster you wish to connect
      redis_host: llm-engine-prod-cache.use1.cache.amazonaws.com
      # s3_bucket [required] is the S3 bucket you wish to connect 
      s3_bucket: "llm-engine"
    launch:
      # endpoint_namespace [required] is K8s namespace the endpoints will be created in
      endpoint_namespace: llm-engine
      # cache_redis_url [required] is the full url for the redis cluster you wish to connect
      cache_redis_url: redis://llm-engine-prod-cache.use1.cache.amazonaws.com:6379/15
      # s3_file_llm_fine_tuning_job_repository [required] is the S3 URI for the S3 bucket/key that you wish to save fine-tuned assests
      s3_file_llm_fine_tuning_job_repository: "s3://llm-engine/llm-ft-job-repository"
      # dd_trace_enabled specifies whether to enable datadog tracing, datadog must be installed in the cluster
      dd_trace_enabled: false
      istio_enabled: true

      # Asynchronous endpoints configs (coming soon)
      sqs_profile: default
      # sqs_queue_policy_template [required] is the IAM policy template for SQS queue for async endpoints.
      sqs_queue_policy_template: >
        {
          "Version": "2012-10-17",
          "Id": "__default_policy_ID",
          "Statement": [
            {
              "Sid": "__owner_statement",
              "Effect": "Allow",
              "Principal": {
                "AWS": "arn:aws:iam::000000000000:root"
              },
              "Action": "sqs:*",
              "Resource": "arn:aws:sqs:us-east-1:000000000000:${queue_name}"
            },
            {
              "Effect": "Allow",
              "Principal": {
                "AWS": "arn:aws:iam::000000000000:role/k8s-main-llm-engine"
              },
              "Action": "sqs:*",
              "Resource": "arn:aws:sqs:us-east-1:000000000000:${queue_name}"
            }
          ]
        }

      sqs_queue_tag_template: >
        {
          "Spellbook-Serve-Endpoint-Id": "${endpoint_id}",
          "Spellbook-Serve-Endpoint-Name": "${endpoint_name}",
          "Spellbook-Serve-Endpoint-Created-By": "${endpoint_created_by}"
        }
      billing_queue_arn: "unused"
      model_primitive_host: "unused"
      hf_user_fine_tuned_weights_prefix: "s3://llm-engine/fine_tuned_weights"

      tgi_repository: "text-generation-inference"
      vllm_repository: "vllm"
      lightllm_repository: "lightllm"
      tensorrt_llm_repository: "tensorrt-llm"
      user_inference_base_repository: "launch/inference"
      user_inference_pytorch_repository: "launch/inference/pytorch"
      user_inference_tensorflow_repository: "launch/inference/tf"
      docker_image_layer_cache_repository: "launch-docker-build-cache"

# Triton enhanced endpoints (coming soon)
triton:
  image:
    repository: 000000000000.dkr.ecr.us-west-2.amazonaws.com/std-ml-srv
    tag: e83eccbc8959f90ebbe4bda618b61ec6ee2d8394-triton

# imageCache specifies the image cache configuration for faster endpoint auto-scaling (coming soon)
imageCache:
  devices:
    - name: cpu
      nodeSelector:
        cpu-only: "true"
    - name: a10
      nodeSelector:
        k8s.amazonaws.com/accelerator: nvidia-ampere-a10
      tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
    - name: a100
      nodeSelector:
        k8s.amazonaws.com/accelerator: nvidia-ampere-a100
      tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
    - name: t4
      nodeSelector:
        k8s.amazonaws.com/accelerator: nvidia-tesla-t4
      tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"

# celeryBrokerType specifies the celery broker type for async endpoints, either "sqs" or "elasticache"
celeryBrokerType: sqs
